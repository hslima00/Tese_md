{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TODO's","text":""},{"location":"#please-edit-this-sheet-to-give-me-todos","title":"Please edit this sheet to give me TODOs","text":""},{"location":"Reuni%C3%B5es/","title":"1-Mar-24","text":"<p>PPT: 01MAR24</p> <p>YOLOv9 paper released on 21FEV24</p> <ul> <li>yolov9 + atrativo </li> <li>meter v9 a funcionar na maquina ciafa</li> <li>o q \u00e9 q vamos ganhar em rela\u00e7\u00e3o ao v8 ?</li> <li>perceber se v9 serve os nossos prpopositos ?</li> <li>exemplo minimo a correr no ciafa </li> <li>adaotar escrito para tese </li> </ul>"},{"location":"Reuni%C3%B5es/#segunda-reuni\u00e3o","title":"Segunda reuni\u00e3o","text":"<ul> <li>03-Mar-24: eu percebi q estava a ser parvo quando disse \"ah e tal o yolov9 n tem segmenta\u00e7\u00e3o ainda, ent\u00e3o talvez seja melhor ficar com o yolov8\"</li> <li>para fazer o autolabelling n\u00f3s apenas precisamos das capacidades do YOLO para fazer object detection, e por sua vez as \"bounding boxes\", portanto d\u00e1 para usar YOLOv9</li> </ul>"},{"location":"Reuni%C3%B5es/#22-mar-24","title":"22-Mar-24","text":"Cenas a fazer <ul> <li>equilibrio entre quantidade  e qualidade  de images</li> <li>selecionar datasets com o mais parecido com o que temos</li> <li>depois do YOLOv9 filtrar a qualidade do q vai para o SAM. basicamente ver se as bounding boxes est\u00e3o boas para ir para o SAM e excluir o que t\u00e1 errado</li> <li>ver ferramenta de anota\u00e7\u00e3o do roboflow para segmenta\u00e7\u00e3o (Human annotator)</li> <li>unified dataset: </li> <li>prioridade UAV/aereo, </li> <li>identificar um test set de apenas imagens aereas anotadas e q n\u00e3o estejam no treino</li> <li>identificar outro test set com imagens genericas/ground anotadas sem estar no train set</li> <li>meter tb falso negativo</li> <li>treino pode ter coisas que n\u00e3o s\u00e3o aereo florestal</li> <li>~1000 test set (a\u00e9reas) (linha 7)</li> <li>~1000 test set + x (linha 8)</li> <li>datasets com frames fazer uma amostragem</li> <li>weights and biases </li> </ul>"},{"location":"ciafa/","title":"CIAFA","text":"<p>Aqui meto cenas relativas ao hardware que est\u00e1 na CIAFA, por exemplo que libs \u00e9 que foram instaladas etc.</p>"},{"location":"ciafa/#yolov9","title":"YOLOv9","text":"<p>04-Mar-24</p>"},{"location":"ciafa/#activating-venv-and-installed-libs","title":"Activating venv and installed libs","text":"BashInstalled in venv <pre><code>source .venv/bin/activate\n</code></pre> <p>I installed everything inside the <code>requirements.txt</code> given in the YOLOv9 repo using the command: </p> Installing requirements.txt<pre><code>pip install -r requirements.txt\n</code></pre> <p>The contents of this file are:  <pre><code>    # requirements\n    # Usage: pip install -r requirements.txt\n\n    # Base ------------------------------------------------------------------------\n    gitpython\n    ipython\n    matplotlib&gt;=3.2.2\n    numpy&gt;=1.18.5\n    opencv-python&gt;=4.1.1\n    Pillow&gt;=7.1.2\n    psutil\n    PyYAML&gt;=5.3.1\n    requests&gt;=2.23.0\n    scipy&gt;=1.4.1\n    thop&gt;=0.1.1\n    torch&gt;=1.7.0\n    torchvision&gt;=0.8.1\n    tqdm&gt;=4.64.0\n    # protobuf&lt;=3.20.1\n\n    # Logging ---------------------------------------------------------------------\n    tensorboard&gt;=2.4.1\n    # clearml&gt;=1.2.0\n    # comet\n\n    # Plotting --------------------------------------------------------------------\n    pandas&gt;=1.1.4\n    seaborn&gt;=0.11.0\n\n    # Export ----------------------------------------------------------------------\n    # coremltools&gt;=6.0\n    # onnx&gt;=1.9.0\n    # onnx-simplifier&gt;=0.4.1\n    # nvidia-pyindex\n    # nvidia-tensorrt\n    # scikit-learn&lt;=1.1.2\n    # tensorflow&gt;=2.4.1\n    # tensorflowjs&gt;=3.9.0\n    # openvino-dev\n\n    # Deploy ----------------------------------------------------------------------\n    # tritonclient[all]~=2.24.0\n\n    # Extras ----------------------------------------------------------------------\n    # mss\n    albumentations&gt;=1.0.3\n    pycocotools&gt;=2.0\n</code></pre></p>"},{"location":"datasets/","title":"Datasets sheet","text":""},{"location":"links/","title":"01-Mar-24","text":"<p>YOLOv9 paper</p> <p>Overleaf Tese (view only)</p>"},{"location":"workplan/","title":"Workplan","text":""},{"location":"yolov9/","title":"YOLOv9","text":""},{"location":"yolov9/#22-mar-24","title":"22-Mar-24","text":""},{"location":"yolov9/#treino-numa-custom-dataset","title":"Treino numa custom dataset","text":"<p>O YOLOv9 foi treinado com uma dataset que arranjei no Roboflow: VOCfinaldataset [acedido a 2024-01-20 12:04am]</p>"},{"location":"yolov9/#resultado-do-treino","title":"Resultado do treino","text":"Confusion matrix F1_curve labels_correlogram labels Precision confidence curve Precision-Recall curve recall confidence results"},{"location":"yolov9/#infer\u00eancia-em-custom-dataset","title":"Infer\u00eancia em custom dataset","text":"Exemplo de uma infer\u00eancia<pre><code>python detect.py --weights ../fire_1_best.pt --conf 0.1 --source ../../FIREFRONT_datasets_release/BA_Fire_Smoke_Multilabel/BA_Fire_Smoke_Multilabel_v1/test --device 0\n</code></pre> <p>Resultado da inferencia</p> <p>O resultado da infer\u00eancia pode ser visto aqui (apenas partilhado com orientador e co-orientador) Note-se que existem muitas inferencias erradas, ent\u00e3o o modelo ter\u00e1 que ser re-treinado com mais imagens. </p>"},{"location":"yolov9/#outputs-do-yolov9","title":"Outputs do YOLOv9","text":"<p>Os outputs quando se faz uma infer\u00eancia <code>detect.py</code> s\u00e3o imagens com as bounding boxes:</p> Exemplo de imagem de uma infer\u00eancia de uma images da dataset 'BA_Fire_Smoke_Multilabel': <p> </p> <p>Por\u00e9m n\u00f3s queremos ter o output como coordenadas para o usar para o SAM. Para este fim foi metido a <code>true</code> a seguinte linha: </p> In 'detect.py' line 34<pre><code>save_txt=True,  # save results to *.txt\n</code></pre> <p>que guarda resultados do tipo: </p> Label<pre><code>0 0.371875 0.34921875 0.48125 0.68984375\n0 0.671875 0.45078125 0.16171875 0.58203125\n0 0.903125 0.3703125 0.1765625 0.4296875\n</code></pre> <p>O primeiro digito correponde \u00e0 classe <code>0 = fire</code> (podia ser <code>1 = smoke</code>, <code>2 = other</code>) e os restantes s\u00e3o as coordenadas da bounding box.</p>"}]}