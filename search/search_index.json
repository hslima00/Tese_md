{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TODO's","text":""},{"location":"#please-edit-this-sheet-to-give-me-todos","title":"Please edit this sheet to give me TODOs","text":""},{"location":"Reuni%C3%B5es/","title":"1-Mar-24","text":"<p>PPT: 01MAR24</p> <p>YOLOv9 paper released on 21FEV24</p> <ul> <li>yolov9 + atrativo </li> <li>meter v9 a funcionar na maquina ciafa</li> <li>o q \u00e9 q vamos ganhar em rela\u00e7\u00e3o ao v8 ?</li> <li>perceber se v9 serve os nossos prpopositos ?</li> <li>exemplo minimo a correr no ciafa </li> <li>adaotar escrito para tese </li> </ul>"},{"location":"Reuni%C3%B5es/#segunda-reuni\u00e3o","title":"Segunda reuni\u00e3o","text":"<ul> <li>03-Mar-24: eu percebi q estava a ser parvo quando disse \"ah e tal o yolov9 n tem segmenta\u00e7\u00e3o ainda, ent\u00e3o talvez seja melhor ficar com o yolov8\"</li> <li>para fazer o autolabelling n\u00f3s apenas precisamos das capacidades do YOLO para fazer object detection, e por sua vez as \"bounding boxes\", portanto d\u00e1 para usar YOLOv9</li> </ul>"},{"location":"Reuni%C3%B5es/#22-mar-24","title":"22-Mar-24","text":"Cenas a fazer <ul> <li>equilibrio entre quantidade  e qualidade  de images</li> <li>selecionar datasets com o mais parecido com o que temos</li> <li>depois do YOLOv9 filtrar a qualidade do q vai para o SAM. basicamente ver se as bounding boxes est\u00e3o boas para ir para o SAM e excluir o que t\u00e1 errado</li> <li>ver ferramenta de anota\u00e7\u00e3o do roboflow para segmenta\u00e7\u00e3o (Human annotator)</li> <li>unified dataset: </li> <li>prioridade UAV/aereo, </li> <li>identificar um test set de apenas imagens aereas anotadas e q n\u00e3o estejam no treino</li> <li>identificar outro test set com imagens genericas/ground anotadas sem estar no train set</li> <li>meter tb falso negativo</li> <li>treino pode ter coisas que n\u00e3o s\u00e3o aereo florestal</li> <li>~1000 test set (a\u00e9reas) (linha 7)</li> <li>~1000 test set + x (linha 8)</li> <li>datasets com frames fazer uma amostragem</li> <li>weights and biases </li> </ul>"},{"location":"ciafa/","title":"CIAFA","text":"<p>Aqui meto cenas relativas ao hardware que est\u00e1 na CIAFA, por exemplo que libs \u00e9 que foram instaladas etc.</p>"},{"location":"ciafa/#yolov9","title":"YOLOv9","text":"<p>04-Mar-24</p>"},{"location":"ciafa/#activating-venv-and-installed-libs","title":"Activating venv and installed libs","text":"BashInstalled in venv <pre><code>source .venv/bin/activate\n</code></pre> <p>I installed everything inside the <code>requirements.txt</code> given in the YOLOv9 repo using the command: </p> Installing requirements.txt<pre><code>pip install -r requirements.txt\n</code></pre> <p>The contents of this file are:  <pre><code>    # requirements\n    # Usage: pip install -r requirements.txt\n\n    # Base ------------------------------------------------------------------------\n    gitpython\n    ipython\n    matplotlib&gt;=3.2.2\n    numpy&gt;=1.18.5\n    opencv-python&gt;=4.1.1\n    Pillow&gt;=7.1.2\n    psutil\n    PyYAML&gt;=5.3.1\n    requests&gt;=2.23.0\n    scipy&gt;=1.4.1\n    thop&gt;=0.1.1\n    torch&gt;=1.7.0\n    torchvision&gt;=0.8.1\n    tqdm&gt;=4.64.0\n    # protobuf&lt;=3.20.1\n\n    # Logging ---------------------------------------------------------------------\n    tensorboard&gt;=2.4.1\n    # clearml&gt;=1.2.0\n    # comet\n\n    # Plotting --------------------------------------------------------------------\n    pandas&gt;=1.1.4\n    seaborn&gt;=0.11.0\n\n    # Export ----------------------------------------------------------------------\n    # coremltools&gt;=6.0\n    # onnx&gt;=1.9.0\n    # onnx-simplifier&gt;=0.4.1\n    # nvidia-pyindex\n    # nvidia-tensorrt\n    # scikit-learn&lt;=1.1.2\n    # tensorflow&gt;=2.4.1\n    # tensorflowjs&gt;=3.9.0\n    # openvino-dev\n\n    # Deploy ----------------------------------------------------------------------\n    # tritonclient[all]~=2.24.0\n\n    # Extras ----------------------------------------------------------------------\n    # mss\n    albumentations&gt;=1.0.3\n    pycocotools&gt;=2.0\n</code></pre></p>"},{"location":"datasets/","title":"Datasets sheet","text":""},{"location":"links/","title":"01-Mar-24","text":"<p>YOLOv9 paper</p> <p>Overleaf Tese (view only)</p>"},{"location":"pipeline/","title":"Pipeline","text":""},{"location":"pipeline/#output-do-yolov9","title":"Output do YOLOv9","text":"Output normalizado xywh<pre><code>0   0.371875  0.34921875   0.48125     0.68984375 \n0   0.671875  0.45078125   0.16171875  0.58203125 \n0   0.903125  0.3703125    0.1765625   0.4296875\n</code></pre> <p>Primeiro digito \u00e9 a <code>classe</code> e os restantes s\u00e3o as coordenadas <code>x,y,w,h</code> normalizadas.</p>"},{"location":"pipeline/#input-do-sam-como-bounding-box","title":"Input do SAM como bounding box","text":"Apenas uma bounding boxMais do que uma bounding box Bounding box xyxy<pre><code>input_box = np.array([425, 600, 700, 875])\ninput_point = np.array([[575, 750]])\ninput_label = np.array([0])\n</code></pre> <p>As coordenadas que o SAM recebe s\u00e3o do formato <code>xyxy</code> n\u00e3o normalizadas. Logo o seguinte c\u00f3digo foi escrito para fazer a convers\u00e3o das cordenadas do YOLOv9: </p> Converter coordenadas<pre><code>def normalize_to_denormalize(x, y, w, h, image_width, image_height):\n    # Convert normalized coordinates to denormalized coordinates\n    x1 = int(x * image_width)\n    y1 = int(y * image_height)\n    x2 = int((x + w) * image_width)\n    y2 = int((y + h) * image_height)\n    \n    return x1, y1, x2, y2\n\n# Example usage:\nnormalized_coords = (0.371875,  0.34921875,   0.48125,     0.68984375)  # Example normalized coordinates (x, y, w, h)\nimage_width = 800  # Example image width\nimage_height = 600  # Example image height\n\nx1, y1, x2, y2 = normalize_to_denormalize(*normalized_coords, image_width, image_height)\nprint(\"Denormalized coordinates:\", (x1, y1, x2, y2))\n\n# output: (1)\n</code></pre> <ol> <li>Output: Denormalized coordinates: (297, 209, 682, 623)</li> </ol> <p>Para termos o tamanho da imagem fazemos: </p> CodeOutput Tamanho imagem<pre><code>image = cv2.imread('images/truck.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \nh, w, c = image.shape\nprint('width:  ', w)\nprint('height: ', h)\nprint('channel:', c)\n</code></pre> <p> </p>"},{"location":"pipeline/#imagem-rbg---mascara-preto-e-branco","title":"Imagem RBG -&gt; Mascara preto e branco","text":"<p>Tem que se adaptar a fun\u00e7\u00e3o:</p> <p><pre><code>def get_whiteBlack_masks_image(self):\n        masks = self.masks\n        darkImg = np.zeros_like(self.origin_image)\n        image = darkImg.copy()\n\n        np.random.seed(0)\n        if (len(masks) == 0):\n            self.whiteMasks = image\n            return image\n        for mask in masks:\n            if mask['opt'] == \"negative\":\n                image = self.clearMaskWithOriginImg(darkImg, image, mask['mask'])\n            else:\n                image = self.overlay_mask(image, mask['mask'], 0.5, random_color=False)\n\n        self.whiteMasks = image\n        return image\n</code></pre> Source, linha 593.</p>"},{"location":"pipeline/#duvidas-para-a-reuni\u00e3o","title":"Duvidas para a reuni\u00e3o","text":"<ol> <li>Encontrei isto (G-DINO + SAM), devo continuar com a pipeline ou experimento isto? Paper 25JAN2024 Grounded SAM</li> <li>Onde arranjar mais imagens para fazer a dataset? (Uma vez que t\u00eam que ser privadas)</li> </ol>"},{"location":"workplan/","title":"Workplan","text":""},{"location":"yolov9/","title":"YOLOv9","text":""},{"location":"yolov9/#22-mar-24","title":"22-Mar-24","text":""},{"location":"yolov9/#treino-numa-custom-dataset","title":"Treino numa custom dataset","text":"<p>O YOLOv9 foi treinado com uma dataset que arranjei no Roboflow: VOCfinaldataset [acedido a 2024-01-20 12:04am]</p>"},{"location":"yolov9/#resultado-do-treino","title":"Resultado do treino","text":"Confusion matrix F1_curve labels_correlogram labels Precision confidence curve Precision-Recall curve recall confidence results"},{"location":"yolov9/#infer\u00eancia-em-custom-dataset","title":"Infer\u00eancia em custom dataset","text":"Exemplo de uma infer\u00eancia<pre><code>python detect.py --weights ../fire_1_best.pt --conf 0.1 --source ../../FIREFRONT_datasets_release/BA_Fire_Smoke_Multilabel/BA_Fire_Smoke_Multilabel_v1/test --device 0\n</code></pre> <p>Resultado da inferencia</p> <p>O resultado da infer\u00eancia pode ser visto aqui (apenas partilhado com orientador e co-orientador) Note-se que existem muitas inferencias erradas, ent\u00e3o o modelo ter\u00e1 que ser re-treinado com mais imagens. </p>"},{"location":"yolov9/#outputs-do-yolov9","title":"Outputs do YOLOv9","text":"<p>Os outputs quando se faz uma infer\u00eancia <code>detect.py</code> s\u00e3o imagens com as bounding boxes:</p> Exemplo de imagem de uma infer\u00eancia de uma images da dataset 'BA_Fire_Smoke_Multilabel': <p> </p> <p>Por\u00e9m n\u00f3s queremos ter o output como coordenadas para o usar para o SAM. Para este fim foi metido a <code>true</code> a seguinte linha: </p> In 'detect.py' line 34<pre><code>save_txt=True,  # save results to *.txt\n</code></pre> <p>que guarda resultados do tipo: </p> Label<pre><code>0 0.371875 0.34921875 0.48125 0.68984375\n0 0.671875 0.45078125 0.16171875 0.58203125\n0 0.903125 0.3703125 0.1765625 0.4296875\n</code></pre> <p>O primeiro digito correponde \u00e0 classe <code>0 = fire</code> (podia ser <code>1 = smoke</code>, <code>2 = other</code>) e os restantes s\u00e3o as coordenadas da bounding box.</p>"}]}