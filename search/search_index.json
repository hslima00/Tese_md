{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TODO'S","text":""},{"location":"2_reunioes/","title":"Reuni\u00f5es","text":""},{"location":"2_reunioes/#1-mar-24","title":"1-Mar-24","text":"<p>YOLOv9 paper released on 21FEV24</p> <ul> <li>yolov9 + atrativo</li> <li>meter v9 a funcionar na maquina ciafa</li> <li>o q \u00e9 q vamos ganhar em rela\u00e7\u00e3o ao v8 ?</li> <li>perceber se v9 serve os nossos prpopositos ?</li> <li>exemplo minimo a correr no ciafa</li> <li>adotar escrito para tese</li> </ul>"},{"location":"2_reunioes/#8-mar-24","title":"8-Mar-24","text":"<ul> <li>03-Mar-24: eu percebi q estava a ser parvo quando disse \"ah e tal o yolov9 n tem segmenta\u00e7\u00e3o ainda, ent\u00e3o talvez seja melhor ficar com o yolov8\"</li> <li>para fazer o autolabelling n\u00f3s apenas precisamos das capacidades do YOLO para fazer object detection, e por sua vez as \"bounding boxes\", portanto d\u00e1 para usar YOLOv9</li> </ul>"},{"location":"2_reunioes/#22-mar-24","title":"22-Mar-24","text":"<ul> <li>equilibrio entre quantidade e qualidade de images</li> <li>selecionar datasets com o mais parecido com o que temos</li> <li>depois do YOLOv9 filtrar a qualidade do q vai para o SAM. basicamente ver se as bounding boxes est\u00e3o boas para ir para o SAM e excluir o que t\u00e1 errado</li> <li>ver ferramenta de anota\u00e7\u00e3o do roboflow para segmenta\u00e7\u00e3o (Human annotator)</li> <li>unified dataset:</li> <li>prioridade UAV/aereo,</li> <li>identificar um test set de apenas imagens aereas anotadas e q n\u00e3o estejam no treino</li> <li>identificar outro test set com imagens genericas/ground anotadas sem estar no train set</li> <li>meter tb falso negativo</li> <li>treino pode ter coisas que n\u00e3o s\u00e3o aereo florestal</li> <li>~1000 test set (a\u00e9reas) (linha 7)</li> <li>~1000 test set + x (linha 8)</li> <li>datasets com frames fazer uma amostragem</li> <li>weights and biases</li> </ul>"},{"location":"2_reunioes/#26-apr-24","title":"26-Apr-24","text":"<ul> <li>ver anchors do yolov9 (?)</li> <li>ver configuara\u00e7\u00e3o que possa limitar tamanho de bounding box</li> <li>passar pipeline na dataset LK </li> <li>ver yolov8/v9 segmentacao</li> <li>criar tabela com parametros do treino (tamanho de imagem, batch size, data augmentation, lr, epocas)</li> <li>averiguar val a 0</li> <li>guardar configura\u00e7\u00e3o do treino</li> </ul>"},{"location":"2_reunioes/#meeting-with-rashmi","title":"Meeting with Rashmi","text":"<ul> <li>AWS Sagemaker, and EC2:</li> </ul>"},{"location":"2_reunioes/#todo","title":"todo","text":"<p>passar a reuniao para escrita!</p>"},{"location":"3_ciafa/","title":"CIAFA","text":"<p>Aqui meto cenas relativas ao hardware que est\u00e1 na CIAFA, por exemplo que libs \u00e9 que foram instaladas etc.</p>"},{"location":"3_ciafa/#yolov9","title":"YOLOv9","text":"<p>04-Mar-24</p>"},{"location":"3_ciafa/#activating-venv-and-installed-libs","title":"Activating venv and installed libs","text":"BashInstalled in the venv <pre><code>source .venv/bin/activate\n</code></pre> <p>I installed everything inside the <code>requirements.txt</code> given in the YOLOv9 repo using the command:</p> requirements.txt<pre><code>pip install -r requirements.txt\n</code></pre> <p>The content of the <code>requirements.txt</code> file is:</p> <pre><code># requirements\n# Usage: pip install -r requirements.txt\n\n# Base ------------------------------------------------------------------------\ngitpython\nipython\nmatplotlib&gt;=3.2.2\nnumpy&gt;=1.18.5\nopencv-python&gt;=4.1.1\nPillow&gt;=7.1.2\npsutil\nPyYAML&gt;=5.3.1\nrequests&gt;=2.23.0\nscipy&gt;=1.4.1\nthop&gt;=0.1.1\ntorch&gt;=1.7.0\ntorchvision&gt;=0.8.1\ntqdm&gt;=4.64.0\n# protobuf&lt;=3.20.1\n\n# Logging ---------------------------------------------------------------------\ntensorboard&gt;=2.4.1\n# clearml&gt;=1.2.0\n# comet\n\n# Plotting --------------------------------------------------------------------\npandas&gt;=1.1.4\nseaborn&gt;=0.11.0\n\n# Export ----------------------------------------------------------------------\n# coremltools&gt;=6.0\n# onnx&gt;=1.9.0\n# onnx-simplifier&gt;=0.4.1\n# nvidia-pyindex\n# nvidia-tensorrt\n# scikit-learn&lt;=1.1.2\n# tensorflow&gt;=2.4.1\n# tensorflowjs&gt;=3.9.0\n# openvino-dev\n\n# Deploy ----------------------------------------------------------------------\n# tritonclient[all]~=2.24.0\n\n# Extras ----------------------------------------------------------------------\n# mss\nalbumentations&gt;=1.0.3\npycocotools&gt;=2.0\n</code></pre>"},{"location":"4_datasets/","title":"Datasets","text":""},{"location":"5_links/","title":"Links","text":""},{"location":"5_links/#01-mar-24","title":"01-Mar-24","text":"<p>YOLOv9 paper</p> <p>Overleaf Tese (view only)</p>"},{"location":"7_workplan/","title":"Workplan","text":""},{"location":"8_yolov9/","title":"YOLOv9","text":""},{"location":"8_yolov9/#treino-numa-custom-dataset","title":"Treino numa custom dataset","text":"<p>O YOLOv9 foi treinado com uma dataset que arranjei no Roboflow:  VOCfinaldataset [acedido a 2024-01-20 12:04am]</p>"},{"location":"8_yolov9/#resultados-do-treino","title":"Resultados do Treino","text":"Plot dos resultados do treino"},{"location":"8_yolov9/#infer\u00eancia-em-custom-dataset","title":"Infer\u00eancia em custom dataset","text":"Infer\u00eancia<pre><code>python detect.py --weights ../fire_1_best.pt --conf 0.1 --source ../../FIREFRONT_datasets_release/BA_Fire_Smoke_Multilabel/BA_Fire_Smoke_Multilabel_v1/test --device 0\n</code></pre> <p>Resultado da Inferencia</p> <p>O resultado da infer\u00eancia pode ser visto aqui (apenas partilhado com orientador e co-orientador) Note-se que existem muitas inferencias erradas, ent\u00e3o o modelo ter\u00e1 que ser re-treinado com mais imagens.</p>"},{"location":"8_yolov9/#outputs-do-yolov9","title":"Outputs do YOLOv9","text":"<p>Os outputs quando se faz uma infer\u00eancia <code>detect.py</code> s\u00e3o imagens com as bounding boxes:</p> Exemplo de imagem de uma infer\u00eancia de uma images da dataset 'BA_Fire_Smoke_Multilabel': <p> </p> <p>Por\u00e9m n\u00f3s queremos ter o output como coordenadas para o usar para o SAM. Para este fim foi metido a <code>true</code> a seguinte linha:</p> In 'detect.py' line 34<pre><code>save_txt=True,  # save results to *.txt\n</code></pre> <p>que guarda resultados do tipo:</p> Label<pre><code>0 0.371875 0.34921875 0.48125 0.68984375\n0 0.671875 0.45078125 0.16171875 0.58203125\n0 0.903125 0.3703125 0.1765625 0.4296875\n</code></pre> <p>O primeiro digito correponde \u00e0 classe <code>0 = fire</code> (podia ser <code>1 = smoke</code>, <code>2 = other</code>) e os restantes s\u00e3o as coordenadas da bounding box.</p>"},{"location":"9_pipeline/","title":"Pipeline","text":"Output normalizado xywh<pre><code>0   0.371875  0.34921875   0.48125     0.68984375 \n0   0.671875  0.45078125   0.16171875  0.58203125 \n0   0.903125  0.3703125    0.1765625   0.4296875\n</code></pre> <p>Primeiro digito \u00e9 a <code>classe</code> e os restantes s\u00e3o as coordenadas <code>x,y,w,h</code> normalizadas.</p>"},{"location":"9_pipeline/#input-do-sam-como-bounding-box","title":"Input do SAM como bounding box","text":"Apenas uma bounding boxMais do que uma bounding box Bounding box xyxy<pre><code>input_box = np.array([425, 600, 700, 875])\ninput_point = np.array([[575, 750]])\ninput_label = np.array([0])\n</code></pre> <p>#TODO</p> <p>As coordenadas que o SAM recebe s\u00e3o do formato <code>xyxy</code> n\u00e3o normalizadas. Logo o seguinte c\u00f3digo foi escrito para fazer a convers\u00e3o das cordenadas do YOLOv9:</p> Convers\u00e3o de coordenadas<pre><code># Normalized bounding box coordinates\nx_center_norm = 0.44816\ny_center_norm = 0.550328\nwidth_norm = 0.896319\nheight_norm = 0.582786\n\n# Image size\nimage_width = 1929\nimage_height = 1371\n\n# Convert normalized coordinates to denormalized coordinates\nx_min = (x_center_norm - width_norm / 2) * image_width\ny_min = (y_center_norm - height_norm / 2) * image_height\nx_max = (x_center_norm + width_norm / 2) * image_width\ny_max = (y_center_norm + height_norm / 2) * image_height\n\n# Print denormalized bounding box coordinates\nprint(\"x_min:\", x_min)\nprint(\"y_min:\", y_min)\nprint(\"x_max:\", x_max)\nprint(\"y_max:\", y_max)\n</code></pre> <p>Para termos o tamanho da imagem fazemos:</p> CodeOutput Tamanho da imagem<pre><code>image = cv2.imread('images/truck.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \nh, w, c = image.shape\nprint('width:  ', w)\nprint('height: ', h)\nprint('channel:', c)\n</code></pre> <p> <p>Output do c\u00f3digo </p>"},{"location":"9_pipeline/#imagem-rbg---mascara-preto-e-branc","title":"Imagem RBG -&gt; Mascara preto e branc","text":"<p>Tem que se adaptar a fun\u00e7\u00e3o:</p> Convers\u00e3o para mascara preto e branco<pre><code>def get_whiteBlack_masks_image(self):\n        masks = self.masks\n        darkImg = np.zeros_like(self.origin_image)\n        image = darkImg.copy()\n\n        np.random.seed(0)\n        if (len(masks) == 0):\n            self.whiteMasks = image\n            return image\n        for mask in masks:\n            if mask['opt'] == \"negative\":\n                image = self.clearMaskWithOriginImg(darkImg, image, mask['mask'])\n            else:\n                image = self.overlay_mask(image, mask['mask'], 0.5, random_color=False)\n\n        self.whiteMasks = image\n        return image\n</code></pre> <p>Source, linha 593.</p>"},{"location":"9_pipeline/#duvidas-para-a-reuni\u00e3o","title":"Duvidas para a reuni\u00e3o","text":"<ul> <li>D: Encontrei isto (G-DINO + SAM), devo continuar com a pipeline ou experimento isto? Paper 25JAN2024 Grounded SAM<ul> <li>R: Continuar com o que temos, e depois ver se podemos usar outra coisa</li> </ul> </li> <li>D: Onde arranjar mais imagens para fazer a dataset? (Uma vez que t\u00eam que ser privadas)<ul> <li>R: Provavelmente FAP tem mais imagens, mas t\u00eam que ser pedidas e provavelmente n\u00e3o ser\u00e3o muitas</li> </ul> </li> </ul>"},{"location":"9_pipeline/#18-apr-24","title":"18-Apr-24","text":"<ul> <li> Ler output do YOLOv9</li> <li> Normalizar coordenadas de <code>xywh</code> para <code>xyxy</code></li> <li> Meter tudo numa estrutura de dados do tipo:</li> </ul> Estrutura de dados<pre><code>data = {\n    'img_path': 'path/to/image.png',\n    'file_name': 'image.png',\n    'bounding_box_data': [\n        {\n            'class_id': 0,\n            'bbox_data': [x1, y1, x2, y2]\n        },\n        {\n            'class_id': 1,\n            'bbox_data': [x1, y1, x2, y2]\n        },\n        ...\n    ]\n}\n</code></pre> <ul> <li> Resolvido o problema de, quando o YOLOv8 n\u00e3o deteta nada, n\u00e3o cria a label.txt, ent\u00e3o se o a imagem existir, vai criar um .txt vazio com o nome da imagem </li> <li> Transformar em m\u00e1scara preto e branco </li> </ul> Infer\u00eancia feita pela pipeline <p> img_1675 </p> <p>Transforma\u00e7\u00e3o em mascara preto e branco</p> <p> Black and white mask </p> <ul> <li> Passar para mais do que uma box</li> </ul> Mais do que uma box <p> </p> <ul> <li> Tamb\u00e9m \u00e9 possivel guardar individualmente cada mascara</li> </ul> Mascaras individuais <p> </p> <p>TODO: separar em pastas por classe (fumo e fogo)</p>"},{"location":"9_pipeline/#22-apr-24","title":"22-Apr-24","text":"<ul> <li> Testar com imagens ground truth</li> </ul>"}]}