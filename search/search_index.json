{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"1_todos/","title":"TODO'S","text":""},{"location":"2_reunioes/","title":"Reuni\u00f5es","text":""},{"location":"2_reunioes/#1-mar-24","title":"1-Mar-24","text":"<p>YOLOv9 paper released on 21FEV24</p> <ul> <li>yolov9 + atrativo</li> <li>meter v9 a funcionar na maquina ciafa</li> <li>o q \u00e9 q vamos ganhar em rela\u00e7\u00e3o ao v8 ?</li> <li>perceber se v9 serve os nossos prpopositos ?</li> <li>exemplo minimo a correr no ciafa</li> <li>adotar escrito para tese</li> </ul>"},{"location":"2_reunioes/#8-mar-24","title":"8-Mar-24","text":"<ul> <li>03-Mar-24: eu percebi q estava a ser parvo quando disse \"ah e tal o yolov9 n tem segmenta\u00e7\u00e3o ainda, ent\u00e3o talvez seja melhor ficar com o yolov8\"</li> <li>para fazer o autolabelling n\u00f3s apenas precisamos das capacidades do YOLO para fazer object detection, e por sua vez as \"bounding boxes\", portanto d\u00e1 para usar YOLOv9</li> </ul>"},{"location":"2_reunioes/#22-mar-24","title":"22-Mar-24","text":"<ul> <li>equilibrio entre quantidade e qualidade de images</li> <li>selecionar datasets com o mais parecido com o que temos</li> <li>depois do YOLOv9 filtrar a qualidade do q vai para o SAM. basicamente ver se as bounding boxes est\u00e3o boas para ir para o SAM e excluir o que t\u00e1 errado</li> <li>ver ferramenta de anota\u00e7\u00e3o do roboflow para segmenta\u00e7\u00e3o (Human annotator)</li> <li>unified dataset:</li> <li>prioridade UAV/aereo,</li> <li>identificar um test set de apenas imagens aereas anotadas e q n\u00e3o estejam no treino</li> <li>identificar outro test set com imagens genericas/ground anotadas sem estar no train set</li> <li>meter tb falso negativo</li> <li>treino pode ter coisas que n\u00e3o s\u00e3o aereo florestal</li> <li>~1000 test set (a\u00e9reas) (linha 7)</li> <li>~1000 test set + x (linha 8)</li> <li>datasets com frames fazer uma amostragem</li> <li>weights and biases</li> </ul>"},{"location":"3_ciafa/","title":"CIAFA","text":"<p>Aqui meto cenas relativas ao hardware que est\u00e1 na CIAFA, por exemplo que libs \u00e9 que foram instaladas etc.</p>"},{"location":"3_ciafa/#yolov9","title":"YOLOv9","text":"<p>04-Mar-24</p>"},{"location":"3_ciafa/#activating-venv-and-installed-libs","title":"Activating venv and installed libs","text":"BashInstalled in the venv <pre><code>source .venv/bin/activate\n</code></pre> <p>I installed everything inside the <code>requirements.txt</code> given in the YOLOv9 repo using the command:</p> requirements.txt<pre><code>pip install -r requirements.txt\n</code></pre> <p>The content of the <code>requirements.txt</code> file is:</p> <pre><code># requirements\n# Usage: pip install -r requirements.txt\n\n# Base ------------------------------------------------------------------------\ngitpython\nipython\nmatplotlib&gt;=3.2.2\nnumpy&gt;=1.18.5\nopencv-python&gt;=4.1.1\nPillow&gt;=7.1.2\npsutil\nPyYAML&gt;=5.3.1\nrequests&gt;=2.23.0\nscipy&gt;=1.4.1\nthop&gt;=0.1.1\ntorch&gt;=1.7.0\ntorchvision&gt;=0.8.1\ntqdm&gt;=4.64.0\n# protobuf&lt;=3.20.1\n\n# Logging ---------------------------------------------------------------------\ntensorboard&gt;=2.4.1\n# clearml&gt;=1.2.0\n# comet\n\n# Plotting --------------------------------------------------------------------\npandas&gt;=1.1.4\nseaborn&gt;=0.11.0\n\n# Export ----------------------------------------------------------------------\n# coremltools&gt;=6.0\n# onnx&gt;=1.9.0\n# onnx-simplifier&gt;=0.4.1\n# nvidia-pyindex\n# nvidia-tensorrt\n# scikit-learn&lt;=1.1.2\n# tensorflow&gt;=2.4.1\n# tensorflowjs&gt;=3.9.0\n# openvino-dev\n\n# Deploy ----------------------------------------------------------------------\n# tritonclient[all]~=2.24.0\n\n# Extras ----------------------------------------------------------------------\n# mss\nalbumentations&gt;=1.0.3\npycocotools&gt;=2.0\n</code></pre>"},{"location":"4_datasets/","title":"Datasets","text":""},{"location":"5_links/","title":"Links","text":""},{"location":"5_links/#01-mar-24","title":"01-Mar-24","text":"<p>YOLOv9 paper</p> <p>Overleaf Tese (view only)</p>"},{"location":"7_workplan/","title":"Workplan","text":""},{"location":"8_yolov9/","title":"YOLOv9","text":""},{"location":"8_yolov9/#treino-numa-custom-dataset","title":"Treino numa custom dataset","text":"<p>O YOLOv9 foi treinado com uma dataset que arranjei no Roboflow:  VOCfinaldataset [acedido a 2024-01-20 12:04am]</p>"},{"location":"8_yolov9/#resultados-do-treino","title":"Resultados do Treino","text":"Plot dos resultados do treino"},{"location":"8_yolov9/#infer\u00eancia-em-custom-dataset","title":"Infer\u00eancia em custom dataset","text":"Infer\u00eancia<pre><code>python detect.py --weights ../fire_1_best.pt --conf 0.1 --source ../../FIREFRONT_datasets_release/BA_Fire_Smoke_Multilabel/BA_Fire_Smoke_Multilabel_v1/test --device 0\n</code></pre> <p>Resultado da Inferencia</p> <p>O resultado da infer\u00eancia pode ser visto aqui (apenas partilhado com orientador e co-orientador) Note-se que existem muitas inferencias erradas, ent\u00e3o o modelo ter\u00e1 que ser re-treinado com mais imagens.</p>"},{"location":"8_yolov9/#outputs-do-yolov9","title":"Outputs do YOLOv9","text":"<p>Os outputs quando se faz uma infer\u00eancia <code>detect.py</code> s\u00e3o imagens com as bounding boxes:</p> Exemplo de imagem de uma infer\u00eancia de uma images da dataset 'BA_Fire_Smoke_Multilabel': <p> </p> <p>Por\u00e9m n\u00f3s queremos ter o output como coordenadas para o usar para o SAM. Para este fim foi metido a <code>true</code> a seguinte linha:</p> In 'detect.py' line 34<pre><code>save_txt=True,  # save results to *.txt\n</code></pre> <p>que guarda resultados do tipo:</p> Label<pre><code>0 0.371875 0.34921875 0.48125 0.68984375\n0 0.671875 0.45078125 0.16171875 0.58203125\n0 0.903125 0.3703125 0.1765625 0.4296875\n</code></pre> <p>O primeiro digito correponde \u00e0 classe <code>0 = fire</code> (podia ser <code>1 = smoke</code>, <code>2 = other</code>) e os restantes s\u00e3o as coordenadas da bounding box.</p>"},{"location":"9_pipeline/","title":"Pipeline","text":"Output normalizado xywh<pre><code>0   0.371875  0.34921875   0.48125     0.68984375 \n0   0.671875  0.45078125   0.16171875  0.58203125 \n0   0.903125  0.3703125    0.1765625   0.4296875\n</code></pre> <p>Primeiro digito \u00e9 a <code>classe</code> e os restantes s\u00e3o as coordenadas <code>x,y,w,h</code> normalizadas.</p>"},{"location":"9_pipeline/#input-do-sam-como-bounding-box","title":"Input do SAM como bounding box","text":"Apenas uma bounding boxMais do que uma bounding box Bounding box xyxy<pre><code>input_box = np.array([425, 600, 700, 875])\ninput_point = np.array([[575, 750]])\ninput_label = np.array([0])\n</code></pre> <p>#TODO</p> <p>As coordenadas que o SAM recebe s\u00e3o do formato <code>xyxy</code> n\u00e3o normalizadas. Logo o seguinte c\u00f3digo foi escrito para fazer a convers\u00e3o das cordenadas do YOLOv9:</p> Convers\u00e3o de coordenadas<pre><code>def normalize_to_denormalize(x, y, w, h, image_width, image_height):\n    # Convert normalized coordinates to denormalized coordinates\n    x1 = int(x * image_width)\n    y1 = int(y * image_height)\n    x2 = int((x + w) * image_width)\n    y2 = int((y + h) * image_height)\n    \n    return x1, y1, x2, y2\n\n# Example usage:\nnormalized_coords = (0.371875,  0.34921875,   0.48125,     0.68984375)  # Example normalized coordinates (x, y, w, h)\nimage_width = 800  # Example image width\nimage_height = 600  # Example image height\n\nx1, y1, x2, y2 = normalize_to_denormalize(*normalized_coords, image_width, image_height)\nprint(\"Denormalized coordinates:\", (x1, y1, x2, y2))\n\n# output: Denormalized coordinates: (297, 209, 682, 623)\n</code></pre> <p>Para termos o tamanho da imagem fazemos:</p> CodeOutput Tamanho da imagem<pre><code>image = cv2.imread('images/truck.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \nh, w, c = image.shape\nprint('width:  ', w)\nprint('height: ', h)\nprint('channel:', c)\n</code></pre> <p> <p>Output do c\u00f3digo </p>"},{"location":"9_pipeline/#imagem-rbg---mascara-preto-e-branc","title":"Imagem RBG -&gt; Mascara preto e branc","text":"<p>Tem que se adaptar a fun\u00e7\u00e3o:</p> Convers\u00e3o para mascara preto e branco<pre><code>def get_whiteBlack_masks_image(self):\n        masks = self.masks\n        darkImg = np.zeros_like(self.origin_image)\n        image = darkImg.copy()\n\n        np.random.seed(0)\n        if (len(masks) == 0):\n            self.whiteMasks = image\n            return image\n        for mask in masks:\n            if mask['opt'] == \"negative\":\n                image = self.clearMaskWithOriginImg(darkImg, image, mask['mask'])\n            else:\n                image = self.overlay_mask(image, mask['mask'], 0.5, random_color=False)\n\n        self.whiteMasks = image\n        return image\n</code></pre> <p>Source, linha 593.</p>"},{"location":"9_pipeline/#duvidas-para-a-reuni\u00e3o","title":"Duvidas para a reuni\u00e3o","text":"<ul> <li>D: Encontrei isto (G-DINO + SAM), devo continuar com a pipeline ou experimento isto? Paper 25JAN2024 Grounded SAM<ul> <li>R: Continuar com o que temos, e depois ver se podemos usar outra coisa</li> </ul> </li> <li>D: Onde arranjar mais imagens para fazer a dataset? (Uma vez que t\u00eam que ser privadas)<ul> <li>R: Provavelmente FAP tem mais imagens, mas t\u00eam que ser pedidas e provavelmente n\u00e3o ser\u00e3o muitas</li> </ul> </li> </ul>"}]}