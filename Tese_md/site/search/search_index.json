{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TODO'S","text":""},{"location":"10_organizer/","title":"Organizer","text":""},{"location":"10_organizer/#what-does-it-do","title":"What does it do?","text":"<p>It helps moving images (around 7k) to the right place. See the following folder tree structures as an example of initial and final states:</p> From thisTo this <pre><code>FIREFRONT_DATASETS_RELEASE_COPY\n\u251c\u2500\u2500\u2500BA_Fire_Smoke_Multilabel\n\u2502   \u2514\u2500\u2500\u2500BA_Fire_Smoke_Multilabel_v1\n\u2502       \u251c\u2500\u2500\u2500test\n\u2502       \u251c\u2500\u2500\u2500train\n\u2502       \u2514\u2500\u2500\u2500val\n\u251c\u2500\u2500\u2500GP_Fire_Segmentation_Webimages\n\u2502   \u2514\u2500\u2500\u2500GP_Fire_Segmentation_Webimages_v1\n\u2502       \u2514\u2500\u2500\u2500dataset\n\u2502           \u251c\u2500\u2500\u2500fire images\n\u2502           \u2502   \u251c\u2500\u2500\u2500images\n\u2502           \u2502   \u2514\u2500\u2500\u2500masks\n\u2502           \u2514\u2500\u2500\u2500no fire images\n\u251c\u2500\u2500\u2500HH_Gestosa_Fire_Segmentation\n\u2502   \u2514\u2500\u2500\u2500HH_Gestosa_Fire_Segmentation_v1\n\u2502       \u251c\u2500\u2500\u2500images\n\u2502       \u2514\u2500\u2500\u2500labels\n\u251c\u2500\u2500\u2500LK_Fire+Smoke\n\u2502   \u2514\u2500\u2500\u2500LK_Fire+Smoke_v1\n\u2502       \u2514\u2500\u2500\u2500Fire-smoke\n\u251c\u2500\u2500\u2500LK_Smoke_Only\n\u2502   \u2514\u2500\u2500\u2500LK_Smoke_Only_v1\n\u2502       \u2514\u2500\u2500\u2500Smoke\n\u251c\u2500\u2500\u2500LK_Smoke_Segmentation\n\u2502   \u2514\u2500\u2500\u2500LK_Smoke_Segmentation_v1\n\u2502       \u2514\u2500\u2500\u2500Smoke\n\u2502           \u251c\u2500\u2500\u2500images\n\u2502           \u2514\u2500\u2500\u2500masks\n\u251c\u2500\u2500\u2500SF_Bombeiros_Fire\n\u2502   \u2514\u2500\u2500\u2500SF_Bombeiros_Fire_v1\n\u2502       \u251c\u2500\u2500\u2500Fire\n\u2502       \u2514\u2500\u2500\u2500No_fire\n\u2514\u2500\u2500\u2500TV_Youtube_Video_Fire\n    \u2514\u2500\u2500\u2500TV_Youtube_Video_Fire_v1\n        \u2514\u2500\u2500\u2500fire_videos\n            \u251c\u2500\u2500\u2500neg\n            \u2514\u2500\u2500\u2500pos\n</code></pre> <pre><code>HL_FIREFRONT_UNIFIED_DATASET\n\u251c\u2500\u2500\u2500false_negatives\n\u251c\u2500\u2500\u2500fire\n\u2502   \u251c\u2500\u2500\u2500forest\n\u2502   \u2502   \u251c\u2500\u2500\u2500aerial\n\u2502   \u2502   \u2514\u2500\u2500\u2500ground\n\u2502   \u2514\u2500\u2500\u2500urban\n\u2502       \u251c\u2500\u2500\u2500aerial\n\u2502       \u2514\u2500\u2500\u2500ground\n\u251c\u2500\u2500\u2500noisy_images\n\u2502   \u251c\u2500\u2500\u2500bad_quality\n\u2502   \u2502   \u251c\u2500\u2500\u2500fire\n\u2502   \u2502   \u2514\u2500\u2500\u2500smoke\n\u2502   \u2514\u2500\u2500\u2500water_mark\n\u2502       \u251c\u2500\u2500\u2500fire\n\u2502       \u2514\u2500\u2500\u2500smoke\n\u251c\u2500\u2500\u2500other\n\u2514\u2500\u2500\u2500smoke\n    \u251c\u2500\u2500\u2500forest\n    \u2502   \u251c\u2500\u2500\u2500aerial\n    \u2502   \u2514\u2500\u2500\u2500ground\n    \u2514\u2500\u2500\u2500urban\n        \u251c\u2500\u2500\u2500aerial\n        \u2514\u2500\u2500\u2500ground\n</code></pre>"},{"location":"10_organizer/#how-to-use-it","title":"How to use it?","text":"<ol> <li> <p>Create a virtual environment and install the requirements:     <pre><code>python -m venv venv\nsource venv/bin/activate # for linux\nvenv\\Scripts\\activate # for windows\npip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Change the paths in the <code>display.py</code> script</p> <ul> <li> TODO: make it possible to edit a config file and place the desired final tree path</li> </ul> </li> <li>Run the script:     <pre><code>python display.py\n</code></pre></li> <li>According to pre-defined keys, look at the image and decide where is should be moved</li> </ol> <p> Key Folder Key Folder 1 Fire/Forest/Aerial 3 Fire/Urban/Aerial 2 Fire/Forest/Ground 4 Fire/Urban/Ground 5 Noisy Images/Bad Quality/Fire 6 Noisy Images/Bad Quality/Smoke 7 Noisy Images/Water Mark/Fire 8 Noisy Images/Water Mark/Smoke 9 Other q Smoke/Forest/Aerial w Smoke/Forest/Ground e Smoke/Urban/Aerial r Smoke/Urban/Ground t False_Negatives Organizer Running"},{"location":"11_repostas/","title":"11 repostas","text":"<p>Concordo em abosluto com tela preta sobre parte classificada; feito Achei muito interessante a evolu\u00e7\u00e3o da performance com as anota\u00e7\u00f5es. Neste sentido, aqueles n\u00fameros de anota\u00e7\u00f5es s\u00e3o autom\u00e1ticas, manuais ou auto+manual? acho que documentar isso mostrava o valor do processo de anota\u00e7\u00e3o autom\u00e1tico com quality control que introduziste.</p> <p>O loop do quality control \u00e9: </p> <ol> <li>Correr a pipeline YOLO+SAM na dataset inteira; </li> <li>Retirar \\(x\\) imagens random (usando um script) do resultado; </li> <li>Corrigir as \\(x\\) anota\u00e7\u00f5es; </li> <li>Juntar \u00e0s anota\u00e7\u00f5es confirmadas (ver tabela abaixo: corrigi 200 anotacoes na ultima fase e adicionei \u00e0s pr\u00e9-existentes)</li> <li>Treinar um novo modelo YOLO apenas com as anota\u00e7\u00f5es corrigidas </li> </ol> <p>Para calcular o mIoU: </p> <ol> <li>Do ponto 1 da lista anterior comparo as anota\u00e7\u00f5es resultantes da pipeline com pr\u00e9 existentes em duas datasets feitas por alunos \u00e0 m\u00e3o pixel a pixel: GP (fogo) e LK (fumo).  </li> </ol> <p>Tinha um erro no codigo que estava a calcular o mIoU, as tabelas atualizadas s\u00e3o:</p> ID #annotations mIoU (GP - fire) mIoU (all - smoke) mIoU (all - fire) HL_annotation_v1 482 0.5138655180533725 0.4999154278959135 0.5588546292939491 HL_annotation_v2 937 0.6139816386997876 0.6902299747060116 0.5672355475783253 HL_annotation_v3 1475 0.6062920947852118 0.6733769970507978 0.5644811093217798 HL_annotation_v4 1675 0.6142947505829379 0.6801983832873386 0.5538738837693626 <p>Sinceramente, acho que o problema do fumo \u00e9 de muito dif\u00edcil resolu\u00e7\u00e3o e acho que tem pouco valor operacional (isto \u00e9 s\u00f3 a minha opini\u00e3o e pode estar errada). Eu limitaria o esfor\u00e7o nesse dom\u00ednio. O mIoU do fumo, supreendentemente, est\u00e1 superior ao mIoU do fogo. Digo surpreendentemente porque quando revejo as anota\u00e7\u00f5es, dou por mim a corrigir muito mais o fumo do que o fogo. Nota: a dataset do fumo onde calculo o mIoU do fumo tem apenas 96 mascaras enquanto que a dataset do fogo tem 180 mascaras. Acho que \u00e9 muito importante documentares o processo que envolve o onnx, n\u00e3o s\u00f3 para a tese ,mas para a p\u00e1gina. TODO! Como \u00e9 que est\u00e1 a correr o trabalho (prepara\u00e7\u00e3o do test set e escrita do latex)? J\u00e1 agora, confirma o link para o teu overleaf, sff. Eu estava a ver mas o \u00fanico projecto (que eu tenho) com o teu nome, n\u00e3o \u00e9 modificado h\u00e1 um m\u00eas, o \u00edndice n\u00e3o est\u00e1 atualizado, etc. Talvez eu n\u00e3o esteja a ver o certo.</p> <p>O link est\u00e1 certo, n\u00e3o fa\u00e7o altera\u00e7\u00f5es ao documento h\u00e1 muito tempo. Vou come\u00e7ar a fazer agora.  </p> <p>Tenho perdido muito tempo na anota\u00e7\u00e3o da dataset (quer a confirmar as anotacoes corretas, quer a corrigir anota\u00e7\u00f5es). 200 imagens demoram em m\u00e9dia umas \u2158h. </p> <p>A minha sugest\u00e3o \u00e9 dar o bra\u00e7o a torcer na anota\u00e7\u00e3o porque penso n\u00e3o conseguir corrigir TODAS as 7k imagens numa semana, e tenho mesmo de avan\u00e7ar para a parte do backend. </p> <p>A minha sugest\u00e3o \u00e9:  - Acabar a anota\u00e7\u00e3o apenas para a test set (HH_Gestosa + ESQ_991) porque \u00e9 a que precisa de estar feita para o backend.  - Quando estiver a test set feita avan\u00e7ar para o backend</p> <p>Preciso da opini\u00e3o urgente do Sr. Major para esta decis\u00e3o.  Eu gostaria mesmo de acabar a dataset porque n\u00e3o h\u00e1 datasets \"de jeito\" para segmentacao de fogo e fumo e acho que era um contributo muito bom para a comunidade cientifica. Porem fazendo as contas: </p> <p>nr imagens corrigidas: 1675 (23% da dataset toda) nr imagens por corrigir: 5325 M\u00e9dia de 200 imagens/4h -&gt; 108h (ininterruptas) at\u00e9 acabar as 5325. Se dedicar 8h/dia (ininterruptas) apenas para acabar a corre\u00e7\u00e3o das anota\u00e7\u00f5es s\u00e3o cerca de 14 dias, 2 semanas. Vale a pena? </p> <p>Importante: a proxima reuniao com o professor ser\u00e1 \u00e0s 14h30 da proxima segunda feira 29JUL.    </p> <p>Gostaria de marcar uma reuni\u00e3o para esta semana, se tiver disponibilidade para tal?</p> <p>Pe\u00e7o desculpa pela leitura longa</p>"},{"location":"2_reunioes/","title":"2 reunioes","text":"<p>teste! </p>"},{"location":"2_reunioes/#reuni\u00f5es","title":"Reuni\u00f5es","text":""},{"location":"2_reunioes/#1-mar-24","title":"1-Mar-24","text":"<p>YOLOv9 paper released on 21FEV24</p> <ul> <li>yolov9 + atrativo</li> <li>meter v9 a funcionar na maquina ciafa</li> <li>o q \u00e9 q vamos ganhar em rela\u00e7\u00e3o ao v8 ?</li> <li>perceber se v9 serve os nossos prpopositos ?</li> <li>exemplo minimo a correr no ciafa</li> <li>adotar escrito para tese</li> </ul>"},{"location":"2_reunioes/#8-mar-24","title":"8-Mar-24","text":"<ul> <li>03-Mar-24: eu percebi q estava a ser parvo quando disse \"ah e tal o yolov9 n tem segmenta\u00e7\u00e3o ainda, ent\u00e3o talvez seja melhor ficar com o yolov8\"</li> <li>para fazer o autolabelling n\u00f3s apenas precisamos das capacidades do YOLO para fazer object detection, e por sua vez as \"bounding boxes\", portanto d\u00e1 para usar YOLOv9</li> </ul>"},{"location":"2_reunioes/#22-mar-24","title":"22-Mar-24","text":"<ul> <li>equilibrio entre quantidade e qualidade de images</li> <li>selecionar datasets com o mais parecido com o que temos</li> <li>depois do YOLOv9 filtrar a qualidade do q vai para o SAM. basicamente ver se as bounding boxes est\u00e3o boas para ir para o SAM e excluir o que t\u00e1 errado</li> <li>ver ferramenta de anota\u00e7\u00e3o do roboflow para segmenta\u00e7\u00e3o (Human annotator)</li> <li>unified dataset:</li> <li>prioridade UAV/aereo,</li> <li>identificar um test set de apenas imagens aereas anotadas e q n\u00e3o estejam no treino</li> <li>identificar outro test set com imagens genericas/ground anotadas sem estar no train set</li> <li>meter tb falso negativo</li> <li>treino pode ter coisas que n\u00e3o s\u00e3o aereo florestal</li> <li>~1000 test set (a\u00e9reas) (linha 7)</li> <li>~1000 test set + x (linha 8)</li> <li>datasets com frames fazer uma amostragem</li> <li>weights and biases</li> </ul>"},{"location":"2_reunioes/#26-apr-24","title":"26-Apr-24","text":"<ul> <li> ver anchors do yolov9 (?)</li> <li> ver configuara\u00e7\u00e3o que possa limitar tamanho de bounding box</li> <li>\"As for bounding box size constraints, there isn't a direct built-in feature in YOLOv8 to limit detection by minimum or maximum bounding box size during the post-processing steps. However, it is a reasonable feature consideration for quality control in certain applications, and we may consider such additions for future versions.\" source</li> <li> passar pipeline na dataset LK </li> <li> ver yolov8/v9 segmentacao</li> <li> criar tabela com parametros do treino (tamanho de imagem, batch size, data augmentation, lr, epocas)</li> <li> averiguar val a 0</li> <li></li> <li> guardar configura\u00e7\u00e3o do treino</li> </ul>"},{"location":"2_reunioes/#meeting-with-rashmi-27-apr-24","title":"Meeting with Rashmi 27-Apr-24","text":"Meeting <p>Do you upload your code to benchmarks?</p> <p>Yes, the code is uploaded to benchmarks, at least we do in MIT. We make our code open source to help other researches and to make our work reproducible.</p> <p>Info</p> <p>In papers with code there are multiple researches that upload their code and datasets. MIT and University of S. Francisco does this, but not every company or university does, such as CorsicanFire.</p> <p>How do you upload? For example, in a benchmark suite like CityScapes it is expected of the researcher to download a training set, develop a model, upload the model/code and then the model is evaluated by the benchmark suite on the backend. In my third challenge I want to give the option to researchers to distribute their code through different hardware, do you know any frameworks where this is possible?</p> <p>(maybe I didn't make myself very clear here because the answer was not very targeted to the hardware distribution) We usually use Amazon SageMaker <sup>1</sup> and Amazon S3 <sup>2</sup> bucket ecosystem. We upload our dataset do S3 bucket and our code to SageMaker. SageMaker is what runs the code files. This is not a free service </p> <p>Are you familiar with ONNX?</p> <p>Yes ONNX is heavily used. ONNX is also like a framework/platform is which you can have a checkpoint of a format of your model and then you can convert it to any other framework. For example, you can convert a PyTorch model to a TensorFlow model. ONNX is a very good platform for model conversion. This is called platform agnostic, so you can convert a model from one framework to another.</p> <p>For edge devices there is also TensorFlow Lite, which is a framework that is used to convert models to be used in edge devices. ONNX is more for cloud computing.</p> <ul> <li> Look into ONNX</li> <li> Look into TensorFlow Lite</li> </ul> <p>Can you elaborate on Amazon Sagemaker? How does it work?</p> <p>Basically Sagemaker is like when you use Google Colab GPU's. They offer very good GPU's and you can run your code on their GPU's. And S3 is like Google Drive, where you can store your data.</p> <p>You are explaining how your environment, as a developer/researcher, is right? Regarding to this, I was thinking to create a docker image that has pre-installed all the necessary libraries and frameworks to run the code and to make it compatible with the serverside hardware. Do you think this is a good idea?</p> <p>Yes docker is actually a good idea. Docker, Kubernetes and all these kind of platform in which you are adding whichever requirements are needed like the Python version, the libraries etc.</p> <p>Lets imagine that, you as a researcher, want to benchmark a model for fire and smoke segmentation and you come acros with my benchmark online. How would you like/be easier for you to interact with it?</p> <p>Quest\u00e3o para os orientadores Is it even worth it to continue with the benchmark suite if there is already a popular and trusted platform named Kaggle?</p> <p>You need the model from the researchers and you will give them a dataset. This is similar to Kaggle competitions. </p> <p>Unlike Kaggle, I don't know if it is possible to create a public repository where researchers can upload their models (upload the model on Amazon S3). </p> <p>Are there any free alternatives to Amazon S3 and SageMaker?</p> <p>Maybe Google Drive but that wouldnt be very efficient would it?</p> <p>Me: I'm concerned about the size of the models if researchers are uploading them via Google Drive and also the privacy problems they might have on Google/Amazon services.</p> <p>Then, the best option would have to be a local server right? Does your university have a server that you can use?</p> <p>To create the benchmark suite I was thinking to code a full stack application made with NuxtJS and Flask. Do you think this is a good idea?</p> <p>You would have problems with the storing of the models. You cant store them in, for example, a MySQL database because of the size. You would have to store them, either locally on the server, or in a cloud service like Amazon S3.</p> <p>Me: Yes, I think we should store them locally. But we actually, as a benchmark, dont need to store the models and code for that long right? We only need the models to be stored for the time of the evaluation and then we could delete them, right?</p> <p>Rashmi: Yes, you are right. You only need the models for the evaluation. I think your idea of using Docker images. I'm a bit skeptical for a single person to develop a full stack application like this because its very hard to do. You would normally need a team to do this. </p> <p>So the whole pipeline for this all it would be: I'm a researcher, I go to the benchmark suite website, I download the Docker image with the pre-defined libraries that need to run on the server-side I would develop my code. I would upload my code and my model and my results, after tested, would be displayed in a table</p> <p>Yes, it sounds simple but its very complex to implement. </p> <p>On the third challenge, I'm still not sure how could I make the splitting of the model/code throghout different hardware.</p> <p>What do you mean by different hardware? There would be hardware constraints? </p> <p>Me: Yes, a challenge only on a NVIDIA 4090 and another challenge on a NVIDIA Jetson. Then a third challenge where a researcher can split the model across the two hardware.</p> <p>Rashmi: Do you know the power of a NVIDIA Jetson? I'm not sure if it's capable of image processing. They can't process heavy images. What would be the size of the images? Are you aware about all these specs?</p> <p>Me: I'm not sure about what hardware I will use. I know for a fact that I want to make a benchmark for normal GPU's, for edge devices and for a combination of both. What I was asked was to create a dataset for a benchmark suite, and I've been given a bunch of images, some labeled and some not. Then I should preform all the labelling on the images and create a unified dataset. The images I've been given are SUPER random, from frames of a video to aerial images to ground images, images from fireman, images from the web. The size of the images also differs a lot. </p> <p>Rashmi: The image size for researchers is very important. There are images with 15MB and images with 2GB, and you can imagine that you cant process accurately a 2GB image in a model trained with 15MB images. </p> <p>It's crucial to consider the resources available to researchers. Sometimes, they might not have the means to train models or build them from scratch. From my perspective as an evaluator, I'll be assessing their models and benchmarking them. But it's important for researchers to also think about building their own models. (pelo contexto acho que ela estava a falar sobre a necessidade de nos preocuparmos com os recursos em termos de hardware dos investigadores, mas n\u00e3o percebi muito bem. Os pesos n\u00e3o iriam ser os mesmos se o c\u00f3digo e dataset fosse o mesmo?)</p> <p>When discussing the dataset with your professor, consider what it will look like and what hardware resources you'll have access to. This information will be essential for building the front-end application. For instance, if you're planning to create a user interface with upload buttons, you need to know what kind of hardware you'll be working with.</p> <p>If you're dealing with multiple hardware options, things can get tricky. Not all hardware supports inference. For example, while we might use Amazon SageMaker for training models due to its capabilities, we often resort to EC2 instances for inference because they're more cost-effective.</p> <p>So, it's crucial to clarify the hardware resources available, the nature and size of the dataset, and then devise a plan for the full-stack architecture. Given the constraints and costs associated with various platforms like S3, SageMaker, Google Cloud, or Azure, it seems like building a custom full-stack application tailored to your specific needs is the best approach.</p> <p>That's just my perspective on you we should proceed.</p>"},{"location":"2_reunioes/#03-may-24-maj-cruz","title":"03-May-24 (Maj. Cruz)","text":"<ul> <li> establecer o tamanho de imagens da dataset</li> <li>o trabalho dos investigadores no nosso caso tb \u00e9 tratar dos dados </li> <li> o que \u00e9 que a NVIDIA tem para converter um modelo para um edge device</li> <li> come\u00e7ar a escrever o que j\u00e1 foi feito no autolabelling</li> <li> descrever o processo de autolabelling</li> <li> explicar o YOLOv9 e SAM genericamente<ul> <li> explicar as configura\u00e7\u00f5es de cada e explicar o output e input de cada </li> </ul> </li> <li> criar diagramas a explicar (talvez)</li> <li> marcar reuniao c prof. Bernardino</li> </ul> <p>uma ideia por paragrafo </p> <p>Sobre as datasets: </p> <ul> <li> fazer 2 grupos: imagens limpas e imagens contaminadas</li> <li> organizar por tamanho de imagem</li> <li> marcas de agua </li> <li> fazer estatisticas (tipo qual % de imagens tem marca de agua)</li> </ul>"},{"location":"2_reunioes/#06-may-24-prof-bernardino","title":"06-May-24 (Prof. Bernardino)","text":""},{"location":"2_reunioes/#o-que-foi-feito-at\u00e9-agora","title":"O que foi feito at\u00e9 agora:","text":"<ul> <li>Autolabelling tool </li> </ul> <ul> <li> <p>Mostrar resultados da autolabelling tool</p> </li> <li> <p>Ferramenta para organizar as imagens, ver aqui e talvez mostrar. Github</p> </li> <li>Reuni\u00e3o com investigadora afiliada do MIT para perceber +- como \u00e9 que eles fazem as coisas</li> </ul>"},{"location":"2_reunioes/#problemas-a-discutir","title":"Problemas a discutir:","text":"<ul> <li>Tamanho das imagens<ul> <li> n\u00e3o \u00e9 problema</li> </ul> </li> <li>Qualidade das imagens, ou seja, se \u00e9 para contar com imagens contaminadas (marca de agua, imagens com baixa qualidade, etc)<ul> <li>R n\u00e3o meter water marks no test set</li> <li>damos tudo aos investigadores, mas na test set queremos aproximar o m\u00e1ximo \u00e0 realidade</li> <li>focar em dete\u00e7\u00e3o de fogo e fumo a\u00e9rea</li> <li>n\u00e3o usar HH_Gestosa_Fire_Segmentation na training set, \u00e9 uma boa dataset para test set </li> <li> fazer crop das imagens da HH_Gestosa_Fire_Segmentation par an\u00e3o apanhar o aviao</li> <li> ver as especifica\u00e7\u00f5es da dataset, camara usada etc  </li> </ul> </li> <li>Qual \u00e9 o hardware que temos dispon\u00edvel<ul> <li> jetson edge </li> </ul> </li> <li>Como \u00e9 que posso melhorar o YOLOv9 para o nosso caso de uso<ul> <li> experimentar modelo de segmentacao do YOLOv9 piores resultados</li> </ul> </li> <li>IoU m\u00e9dio para a dataset GP_Fire_Segmentation_Webimages_v1 \u00e9 de 0.52</li> <li>IoU m\u00e9dio para a dataset HH_Gestosa_Fire_Segmentation \u00e9 de 0.00 -&gt; YOLO n\u00e3o detetou nada. </li> <li>Problemas do YOLOv9: falsos negativos, bounding boxes muito grandes<ul> <li>Existe maneira de diminuir as bounding boxes? (pelo que estive a ver nos issues do github do YOLOv8 n\u00e3o est\u00e1 implementada uma fun\u00e7\u00e3o do tipo)</li> </ul> </li> <li>Tenho duvidas sobre a segmenta\u00e7ao de fumo por parte do SAM<ul> <li>fazer analise qualitativa, mostrar a varias pessoas a mesma segmentacao e perceber se elas concordam com a segmenta\u00e7\u00e3o. Labels da Lisa t\u00eam dois graus, ambiguo e nao ambiguo</li> </ul> </li> <li>\u00c9 possivel ter uma maquina remota do IST? Estou a usar uma da AFA e \u00e0s vezes tenho problemas</li> </ul> Melhores 5 imagens da dataset GP_Fire_Segmentation_Webimages_v1 Piores 5 imagens"},{"location":"2_reunioes/#09-may-24-maj-cruz","title":"09-May-24 (Maj. Cruz)","text":"<p>Trabalho realizado esta semana: </p> <ul> <li>Avaliar se o YOLOv9-seg \u00e9 melhor que a pipeline YOLOv9-det + SAM<ul> <li> treinado um modelo YOLOv9-seg com uma dataset com 201 imagens (foi o que encontrei preparado para YOLOv9\u2026), o YOLOv9-det foi treinado com 8939\u2026</li> <li> retreinar com nova dataset com 6k images, para ser mais justo</li> </ul> </li> <li>Avaliado modelo treinado YOLOv9-det 8k images 100 epochs, para v\u00e1rias datasets, IoU subiu </li> <li>Detetado erro quando passagem da pipeline pela dataset HH_Gestosa, IoU est\u00e1 agora nos 0.4 relativamente \u00e0 ground truth</li> <li>Passagem da pipeline pela LK_Fire com modelos diferentes, resultados parecem bons </li> </ul> <p>Duvidas: </p> <ul> <li>Ainda \u00e9 possivel arranjar fotos da FAP? </li> </ul> <p>Trabalho Futuro:</p> <ul> <li> Treinar YOLOv9-seg com 3k images</li> <li> Ver se \u00e9 melhor que a pipeline: n\u00e3o \u00e9 melhor</li> <li> Escrever sobre resultados: a escrever, ainda falta compara\u00e7\u00e3o entre pipeline e YOLOv9-seg </li> </ul> <p>Resultados:</p> <p> Dataset Model Method mean IoU GP_Fire_Segmentation_Webimages_v1 fire_best pipeline 0.52 GP_Fire_Segmentation_Webimages_v1 5_out pipeline 0.53 LK_Fire+Smoke_V1 fire_best pipeline no label LK_Fire+Smoke_V1 5_out pipeline no label HH_Gestosa_Fire_Segmentation 5_out pipeline 0.4 GP_Fire_Segmentation_Webimages_v1 results_100 yolov9-seg 0.3 GP_Fire_Segmentation_Webimages_v1 results_200 yolov9-seg 0.33 <p></p>"},{"location":"2_reunioes/#13-may-24-trabalho","title":"13-May-24 (trabalho)","text":"<ul> <li> treinar YOLOv9-seg com 3k images 10 epochs</li> <li> treinar YOLOv9-seg com 3k images 50 epochs</li> <li> escrever sobre resultados</li> <li> mandar mail ao Cmdt 991</li> <li> pensar na forma de organizar as fotos </li> <li> convers\u00e3o entre m\u00e1scaras PB e label txt YOLOv9 <ul> <li>esta convers\u00e3o s\u00f3 ser\u00e1 \u00fatil para retreinar o yolov9 com as imagens que s\u00e3o anotadas pelo ser humano. para isto tenho de ponderar deixar a autolabeling OU arranjar uma forma de passar da m\u00e1scara para uma label txt de bounding box (o que eu acho que \u00e9 muito muito mais complexo)!</li> </ul> </li> </ul>"},{"location":"2_reunioes/#23-may-24","title":"23-May-24","text":"<p>Trabalho realizado:</p> <ul> <li> Pedido de fotos \u00e0 Esq. 991 -&gt; perguntei ao Ten. Lu\u00eds Santos informalmente se a esquadra tinha fotos, e ele disse que sim, por\u00e9m que devia contactar o Cmdt. </li> <li> Pedido formal Sr. Gen. CEMFA</li> <li> Treino YOLOv9-seg<ul> <li> 10 epochs<ul> <li>3k images</li> </ul> </li> <li> 50 epochs<ul> <li>3k images</li> </ul> </li> <li> 100 epochs<ul> <li>200 images</li> <li>3k images</li> </ul> </li> <li> 200 epochs<ul> <li>200 images</li> </ul> </li> </ul> </li> <li> Come\u00e7ar escrita</li> </ul> <p>Resultados YOLOv9-seg:</p> <p> Model IoU 10_epoch_model_3k 0.4048 50_epoch_model_3k 0.429 100_epoch_model_200 0.307 100_epoch_model_3k 0.429 200_epoch_model 0.334 <p></p> Imagens <p> </p> <p> </p> <p> </p> <p>Trabalho futuro:</p> <ul> <li> escrever sobre resultados do YOLOv9-seg</li> <li> criar script para transformar mascaras PB em label txt YOLOv9, para retreinar </li> <li> treinar novo modelo com as imagens que j\u00e1 tenho label</li> <li> come\u00e7ar a fazer a label das imagens</li> <li> criar script para separar as imagens que s\u00e3o aceites e as que n\u00e3o s\u00e3o aceites pelo anotador (tipo, um script que mostre as imagens, as imagens com a dete\u00e7\u00e3o do YOLOv9 e as mascaras de segmenta\u00e7\u00e3o. Depois o anotador s\u00f3 tem de premir uma tecla para decidir se passa ou n\u00e3o. Caso passe, a imagem e a sua respetiva label \u00e9 automaticamente movida para a pasta final da unified dataset, caso n\u00e3o passe, a imagem \u00e9 automaticamente movida para uma pasta de imagens n\u00e3o aceites. Depois desta passagem, retreinar o YOLOv9 novamente com as imagens que foram aceites. Repetir o processo at\u00e9 temros as imagens todas labeled)</li> </ul> <p>Duvidas:</p> <ul> <li>Imagens CA</li> <li>Pq \u00e9 q n\u00e3o \u00e9 boa ideia pedir imagens \u00e0 991? (o pedido j\u00e1 seguiu porque j\u00e1 tinha falado com o DC sobre isso) <ul> <li>ter imagens que n\u00e3o s\u00e3o acessiveis ao publico, ou seja, as que forem cedidas pela 991/CA s\u00e3o uma otima fonte para o test set que ir\u00e1 estar no server side</li> </ul> </li> <li>Perante os resultados, deixo o YOLOv9-seg e foco apenas na pipeline (YOLOv9-det+SAM)? Relembrar que o YOLOv9-det foi treinado com 6k imagens, e o YOLOv9-seg com 3k imagens. Tamb\u00e9m seria muito mais f\u00e1cil (ainda n\u00e3o sei porque ainda n\u00e3o pensei sobre este assunto a fundo) passar as mascaras PB para o YOLOv9-seg visto que aquilo guarda a posi\u00e7\u00e3o dos pixeis, facilmente retirada de uma imagem, enquanto que o YOLOv9-det s\u00e3o coordenadas normalizadas <code>xyhw</code>. </li> </ul>"},{"location":"2_reunioes/#06-jun-24-maj-cruz","title":"06-Jun-24 (Maj. Cruz)","text":"<p>Trabalho realizado:</p> <ul> <li> Unified dataset (HL_firefront)</li> <li> Hypertune de parametros </li> <li> Modelo escolhido: 20_ABR_conf_20 -&gt; aumentar a conf threshold melhorou alguns casos das bbox serem mt grandes. mIoU de 0.456. </li> <li> Comecei a ver a parte do backend e do frontend.</li> <li> Fazer altera\u00e7\u00f5es ao LaTeX apontadas pelo Major   </li> </ul> <p>Trabalho futuro:</p> <ul> <li> Fazer programa que mostra as imagens originais e as labels feitas pela pipeline. Utilizador pode descartar a anota\u00e7\u00e3o ou pode aceitar. As que forem descartadas, quero integrar a lib do labelme no python para que se possa desenhar poligonos onde a anota\u00e7\u00e3o n\u00e3o for boa. Para isto tb tenho de arranjar maneira de passar de mascara B&amp;W para formato labelme, para aproveitar alguma coisa que esteja bem da pipeline e fazer altera\u00e7\u00f5es minimas. </li> </ul> <p>Duvidas/Dificuldades:</p> <ul> <li>Fiquei \"vidrado\" no treino de hypertunning de parametros perdendo algum tempo que podia ter canalizado para o avan\u00e7o da pipeline e/ou da fullstack application.</li> <li>Ns se j\u00e1 mostrei o \"Image Organizer\", mas ainda n avancei muito com aquilo. Est\u00e3o separadas apenas 2000/7500.</li> <li>Imagens 991/CA</li> <li></li> <li> <p>tentar guardar a penultima epoca</p> </li> <li>desdobrar o trabalho futuro em dois</li> <li>labelme \u00e9 um nice to have </li> <li>pensar como \u00e9 que os utilizadores vao submeter </li> <li>focar na app</li> <li>nas mascaras de segmenta\u00e7\u00e3o ter duas cores para classes diferentes</li> <li>pensar em propsota de uniformiza\u00e7\u00e3o </li> <li>ver se mascaras tem 1 ou 3 canais (0,128,255)</li> </ul> <p>Latex:  - explicar como \u00e9 que o yolov9 se encaixa na pipeline - revisao literatura tenta se dizer que x metodologia usar y tecnica  - falar das adaptacoes que tivemos que fazer para fazer a pipeline </p>"},{"location":"2_reunioes/#21-jun-24-prof-bernardino--maj-cruz","title":"21-Jun-24 (Prof. Bernardino + Maj. Cruz)","text":"<p>Notas da reuni\u00e3o:</p> <ul> <li>dividir ground/aerial</li> <li>mudar cor da mascara do fumo </li> <li>n\u00e3o anotar \u00e0 mao as rejected e retreinar com os accepted</li> <li>come\u00e7ar doc de raiz e ir bucar coisas importantes ao pic2 </li> <li>fazer backend prioritario </li> <li>passar unified dataset para segundo plano</li> <li>fazer ciclos e mostrar que o modelo melhora </li> <li>fazer novo cronograma com perpetiva de evolucao do trabalho sem dependencia de terceiros e mandar para orientadores </li> <li>focar na parte do teste </li> <li>ver evolucao do modelo \u00e0 medida que se incrementa a training set problema do tempo aqui, but lets see\u2026  </li> </ul> <p>para a semana</p> <ul> <li> cronograma atualizado     </li> <li> numeros: imagens com labels: para isto preciso de ter o numero total de imagens, ou seja, faltam as imagens da ESQ991. <ul> <li> quantas labels \u00e9 q j\u00e1 tenho feitas   </li> </ul> </li> <li> decis\u00e3o do que \u00e9 q vai ser o test set: o test set vai ser composto por cerca de 30% </li> </ul> <p>IMPORTANTE</p> <ul> <li> Resolver o problema do labelme tirar algumas masks feitas pela pipeline</li> </ul>"},{"location":"2_reunioes/#28-jun-24","title":"28-Jun-24","text":"<ul> <li>6443 FIREFRONT + 534 adquiridas na ESQ.<ul> <li> perguntar se \u00e9 para manter </li> </ul> </li> <li>Labels: 432 + 35 corrigidas = 467 (7.2% | 6.69% c/ 991)</li> <li>Test set: <ul> <li>duas datasets: ground e aereo. </li> <li>no test set aereo devem constar imagens do Gestosa e 991, uma vez que s\u00e3o as que mais se aproximam \u00e0 finalidade do FIREFRONT</li> </ul> </li> </ul>"},{"location":"2_reunioes/#15-jul-24-prof-bernardino","title":"15-Jul-24 (prof. Bernardino)","text":"<ul> <li>mascaras morfologica open/close - opencv: para o fogo, delimitar regioes ignore</li> <li>dividir bbox para ficarem mais pequenas, ter v\u00e1rias instancias de SAM e fazer a jun\u00e7\u00e3o para ignorar o erro</li> <li>substituir gaussian blur por tela preta</li> <li>disponibilizar apenas uma imagem da esq 991 para investigadores terem em conta as caixas pretas e asa do aviao da gestosa </li> <li>codigo de pre-processamento e p\u00f3s deve ser feito dentro de uma layer (para simplificar uniformizacao onnx)</li> <li>averiguar diferentes sizes de imagem e uso de onnx  </li> </ul> <ol> <li> <p>Build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows\u00a0\u21a9</p> </li> <li> <p>Scalable, secure, and versatile object storage solution for diverse data needs.\u00a0\u21a9</p> </li> </ol>"},{"location":"3_ciafa/","title":"CIAFA","text":"<p>Aqui meto cenas relativas ao hardware que est\u00e1 na CIAFA, por exemplo que libs \u00e9 que foram instaladas etc.</p>"},{"location":"3_ciafa/#yolov9","title":"YOLOv9","text":"<p>04-Mar-24</p>"},{"location":"3_ciafa/#activating-venv-and-installed-libs","title":"Activating venv and installed libs","text":"BashInstalled in the venv <pre><code>source .venv/bin/activate\n</code></pre> <p>I installed everything inside the <code>requirements.txt</code> given in the YOLOv9 repo using the command:</p> requirements.txt<pre><code>pip install -r requirements.txt\n</code></pre> <p>The content of the <code>requirements.txt</code> file is:</p> <pre><code># requirements\n# Usage: pip install -r requirements.txt\n\n# Base ------------------------------------------------------------------------\ngitpython\nipython\nmatplotlib&gt;=3.2.2\nnumpy&gt;=1.18.5\nopencv-python&gt;=4.1.1\nPillow&gt;=7.1.2\npsutil\nPyYAML&gt;=5.3.1\nrequests&gt;=2.23.0\nscipy&gt;=1.4.1\nthop&gt;=0.1.1\ntorch&gt;=1.7.0\ntorchvision&gt;=0.8.1\ntqdm&gt;=4.64.0\n# protobuf&lt;=3.20.1\n\n# Logging ---------------------------------------------------------------------\ntensorboard&gt;=2.4.1\n# clearml&gt;=1.2.0\n# comet\n\n# Plotting --------------------------------------------------------------------\npandas&gt;=1.1.4\nseaborn&gt;=0.11.0\n\n# Export ----------------------------------------------------------------------\n# coremltools&gt;=6.0\n# onnx&gt;=1.9.0\n# onnx-simplifier&gt;=0.4.1\n# nvidia-pyindex\n# nvidia-tensorrt\n# scikit-learn&lt;=1.1.2\n# tensorflow&gt;=2.4.1\n# tensorflowjs&gt;=3.9.0\n# openvino-dev\n\n# Deploy ----------------------------------------------------------------------\n# tritonclient[all]~=2.24.0\n\n# Extras ----------------------------------------------------------------------\n# mss\nalbumentations&gt;=1.0.3\npycocotools&gt;=2.0\n</code></pre>"},{"location":"4_datasets/","title":"Datasets","text":""},{"location":"5_links/","title":"Links","text":""},{"location":"5_links/#01-mar-24","title":"01-Mar-24","text":"<p>YOLOv9 paper</p> <p>Overleaf Tese (view only)</p>"},{"location":"7_workplan/","title":"Workplan","text":""},{"location":"8_yolov9/","title":"YOLOv9","text":""},{"location":"8_yolov9/#treino-numa-custom-dataset","title":"Treino numa custom dataset","text":"<p>O YOLOv9 foi treinado com uma dataset que arranjei no Roboflow:  VOCfinaldataset [acedido a 2024-01-20 12:04am]</p>"},{"location":"8_yolov9/#resultados-do-treino","title":"Resultados do Treino","text":"Plot dos resultados do treino"},{"location":"8_yolov9/#infer\u00eancia-em-custom-dataset","title":"Infer\u00eancia em custom dataset","text":"Infer\u00eancia<pre><code>python detect.py --weights ../fire_1_best.pt --conf 0.1 --source ../../FIREFRONT_datasets_release/BA_Fire_Smoke_Multilabel/BA_Fire_Smoke_Multilabel_v1/test --device 0\n</code></pre> <p>Resultado da Inferencia</p> <p>O resultado da infer\u00eancia pode ser visto aqui (apenas partilhado com orientador e co-orientador) Note-se que existem muitas inferencias erradas, ent\u00e3o o modelo ter\u00e1 que ser re-treinado com mais imagens.</p>"},{"location":"8_yolov9/#outputs-do-yolov9","title":"Outputs do YOLOv9","text":"<p>Os outputs quando se faz uma infer\u00eancia <code>detect.py</code> s\u00e3o imagens com as bounding boxes:</p> Exemplo de imagem de uma infer\u00eancia de uma images da dataset 'BA_Fire_Smoke_Multilabel': <p> </p> <p>Por\u00e9m n\u00f3s queremos ter o output como coordenadas para o usar para o SAM. Para este fim foi metido a <code>true</code> a seguinte linha:</p> In 'detect.py' line 34<pre><code>save_txt=True,  # save results to *.txt\n</code></pre> <p>que guarda resultados do tipo:</p> Label<pre><code>0 0.371875 0.34921875 0.48125 0.68984375\n0 0.671875 0.45078125 0.16171875 0.58203125\n0 0.903125 0.3703125 0.1765625 0.4296875\n</code></pre> <p>O primeiro digito correponde \u00e0 classe <code>0 = fire</code> (podia ser <code>1 = smoke</code>, <code>2 = other</code>) e os restantes s\u00e3o as coordenadas da bounding box.</p>"},{"location":"8_yolov9/#yolov9-retrain-02-may-24","title":"YOLOv9 retrain 02-May-24","text":"Retrain<pre><code>python train_dual.py --workers 8 --device 0 --batch 16 --data /home/hslima/hdd/yolov9/yolov9/fire_dataset/data.yaml --img 640 --cfg /home/hslima/hdd/yolov9/yolov9/models/detect/yolov9_custom.yaml --weights /home/hslima/hdd/yolov9/yolov9-e.pt --name fire2 --hyp /home/hslima/hdd/yolov9/yolov9/data/hyps/hyp.scratch-high.yaml --min-items 0 --epochs 100 --close-mosaic 15\n</code></pre>"},{"location":"9_pipeline/","title":"Pipeline","text":"Output normalizado xywh<pre><code>0   0.371875  0.34921875   0.48125     0.68984375 \n0   0.671875  0.45078125   0.16171875  0.58203125 \n0   0.903125  0.3703125    0.1765625   0.4296875\n</code></pre> <p>Primeiro digito \u00e9 a <code>classe</code> e os restantes s\u00e3o as coordenadas <code>x,y,w,h</code> normalizadas.</p>"},{"location":"9_pipeline/#input-do-sam-como-bounding-box","title":"Input do SAM como bounding box","text":"Apenas uma bounding boxMais do que uma bounding box Bounding box xyxy<pre><code>input_box = np.array([425, 600, 700, 875])\ninput_point = np.array([[575, 750]])\ninput_label = np.array([0])\n</code></pre> <p>#TODO</p> <p>As coordenadas que o SAM recebe s\u00e3o do formato <code>xyxy</code> n\u00e3o normalizadas. Logo o seguinte c\u00f3digo foi escrito para fazer a convers\u00e3o das cordenadas do YOLOv9:</p> Convers\u00e3o de coordenadas<pre><code># Normalized bounding box coordinates\nx_center_norm = 0.44816\ny_center_norm = 0.550328\nwidth_norm = 0.896319\nheight_norm = 0.582786\n\n# Image size\nimage_width = 1929\nimage_height = 1371\n\n# Convert normalized coordinates to denormalized coordinates\nx_min = (x_center_norm - width_norm / 2) * image_width\ny_min = (y_center_norm - height_norm / 2) * image_height\nx_max = (x_center_norm + width_norm / 2) * image_width\ny_max = (y_center_norm + height_norm / 2) * image_height\n\n# Print denormalized bounding box coordinates\nprint(\"x_min:\", x_min)\nprint(\"y_min:\", y_min)\nprint(\"x_max:\", x_max)\nprint(\"y_max:\", y_max)\n</code></pre> <p>Para termos o tamanho da imagem fazemos:</p> CodeOutput Tamanho da imagem<pre><code>image = cv2.imread('images/truck.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \nh, w, c = image.shape\nprint('width:  ', w)\nprint('height: ', h)\nprint('channel:', c)\n</code></pre> <p> <p>Output do c\u00f3digo </p>"},{"location":"9_pipeline/#imagem-rbg---mascara-preto-e-branc","title":"Imagem RBG -&gt; Mascara preto e branc","text":"<p>Tem que se adaptar a fun\u00e7\u00e3o:</p> Convers\u00e3o para mascara preto e branco<pre><code>def get_whiteBlack_masks_image(self):\n        masks = self.masks\n        darkImg = np.zeros_like(self.origin_image)\n        image = darkImg.copy()\n\n        np.random.seed(0)\n        if (len(masks) == 0):\n            self.whiteMasks = image\n            return image\n        for mask in masks:\n            if mask['opt'] == \"negative\":\n                image = self.clearMaskWithOriginImg(darkImg, image, mask['mask'])\n            else:\n                image = self.overlay_mask(image, mask['mask'], 0.5, random_color=False)\n\n        self.whiteMasks = image\n        return image\n</code></pre> <p>Source, linha 593.</p>"},{"location":"9_pipeline/#duvidas-para-a-reuni\u00e3o","title":"Duvidas para a reuni\u00e3o","text":"<ul> <li>D: Encontrei isto (G-DINO + SAM), devo continuar com a pipeline ou experimento isto? Paper 25JAN2024 Grounded SAM<ul> <li>R: Continuar com o que temos, e depois ver se podemos usar outra coisa</li> </ul> </li> <li>D: Onde arranjar mais imagens para fazer a dataset? (Uma vez que t\u00eam que ser privadas)<ul> <li>R: Provavelmente FAP tem mais imagens, mas t\u00eam que ser pedidas e provavelmente n\u00e3o ser\u00e3o muitas</li> </ul> </li> </ul>"},{"location":"9_pipeline/#18-apr-24","title":"18-Apr-24","text":"<ul> <li> Ler output do YOLOv9</li> <li> Normalizar coordenadas de <code>xywh</code> para <code>xyxy</code></li> <li> Meter tudo numa estrutura de dados do tipo:</li> </ul> Estrutura de dados<pre><code>data = {\n    'img_path': 'path/to/image.png',\n    'file_name': 'image.png',\n    'bounding_box_data': [\n        {\n            'class_id': 0,\n            'bbox_data': [x1, y1, x2, y2]\n        },\n        {\n            'class_id': 1,\n            'bbox_data': [x1, y1, x2, y2]\n        },\n        ...\n    ]\n}\n</code></pre> <ul> <li> Resolvido o problema de, quando o YOLOv8 n\u00e3o deteta nada, n\u00e3o cria a label.txt, ent\u00e3o se o a imagem existir, vai criar um .txt vazio com o nome da imagem </li> <li> Transformar em m\u00e1scara preto e branco </li> </ul> Infer\u00eancia feita pela pipeline <p> img_1675 </p> <p>Transforma\u00e7\u00e3o em mascara preto e branco</p> <p> Black and white mask </p> <ul> <li> Passar para mais do que uma box</li> </ul> Mais do que uma box <p> </p> <ul> <li> Tamb\u00e9m \u00e9 possivel guardar individualmente cada mascara</li> </ul> Mascaras individuais <p> </p> <p>TODO: separar em pastas por classe (fumo e fogo)</p>"},{"location":"9_pipeline/#22-apr-24","title":"22-Apr-24","text":"<ul> <li> Testar com imagens ground truth</li> </ul>"},{"location":"9_pipeline/#12-jul-24","title":"12-Jul-24","text":"<ul> <li> <p>meter no latex que treinar com poligonos \u00e9 mais eficiente e apresenta melhores resultados para o YOLO </p> </li> <li> <p>v1 -&gt; 45  imagens </p> </li> <li>v2 -&gt; 482 imagens</li> <li>v3 -&gt; 949 imagens</li> </ul> <p>refazer yolo dataset com anotacoes - feito retreinar - feito anotar dataset inteira com novo modelo - feito get results </p>"}]}