<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Thesis notes"><meta name=author content="Hugo Lima"><link href=https://hslima00.github.io/Tese_md/2_reunioes/ rel=canonical><link href=../10_organizer/ rel=prev><link href=../3_ciafa/ rel=next><link rel=icon href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.5.3, mkdocs-material-9.4.8"><title>Reuniões - Tese Lima</title><link rel=stylesheet href=../assets/stylesheets/main.4b4a2bd9.min.css><link rel=stylesheet href=../assets/stylesheets/palette.356b1318.min.css><script src=../assets/pymdownx-extras/material-extra-theme-TVq-kNRT.js type=text/javascript></script><script src=../assets/pymdownx-extras/material-extra-3rdparty-E-i8w1WA.js type=text/javascript></script><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Ubuntu+Mono:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Ubuntu Mono";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../assets/pymdownx-extras/extra-13d1e30d3e.css><link rel=stylesheet href=../assets/pymdownx-extras/extra-ed665380f8.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script> <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../assets/javascripts/glightbox.min.js"></script></head> <body dir=ltr data-md-color-scheme=dracula data-md-color-primary=deep-purple data-md-color-accent=deep-purple> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#reuniões class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> <!-- <span class="twemoji">

<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m23 12-2.44-2.78.34-3.68-3.61-.82-1.89-3.18L12 3 8.6 1.54 6.71 4.72l-3.61.81.34 3.68L1 12l2.44 2.78-.34 3.69 3.61.82 1.89 3.18L12 21l3.4 1.46 1.89-3.18 3.61-.82-.34-3.68L23 12m-10 5h-2v-2h2v2m0-4h-2V7h2v6Z"/></svg>
</span>
9.0 has been released! Check out the <a href="../about/releases/9.0/#9.0">Release Notes</a> for more information and migration tips.
<br>
<a href="../about/contributing/#become-a-sponsor">Sponsorship</a>
is now available!
<span class="twemoji heart-throb">
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M7.655 14.916v-.001h-.002l-.006-.003-.018-.01a22.066 22.066 0 0 1-3.744-2.584C2.045 10.731 0 8.35 0 5.5 0 2.836 2.086 1 4.25 1 5.797 1 7.153 1.802 8 3.02 8.847 1.802 10.203 1 11.75 1 13.914 1 16 2.836 16 5.5c0 2.85-2.044 5.231-3.886 6.818a22.094 22.094 0 0 1-3.433 2.414 7.152 7.152 0 0 1-.31.17l-.018.01-.008.004a.75.75 0 0 1-.69 0Z"/></svg>
</span> --> </div> </aside> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=header.title> <a href=https://hslima00.github.io/Tese_md/ title="Tese Lima" class="md-header__button md-logo" aria-label="Tese Lima"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m19 2-5 4.5v11l5-4.5V2M6.5 5C4.55 5 2.45 5.4 1 6.5v14.66c0 .25.25.5.5.5.1 0 .15-.07.25-.07 1.35-.65 3.3-1.09 4.75-1.09 1.95 0 4.05.4 5.5 1.5 1.35-.85 3.8-1.5 5.5-1.5 1.65 0 3.35.31 4.75 1.06.1.05.15.03.25.03.25 0 .5-.25.5-.5V6.5c-.6-.45-1.25-.75-2-1V19c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5V6.5C10.55 5.4 8.45 5 6.5 5Z"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Tese Lima </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Reuniões </span> </div> </div> </div> <div class=md-header__options> <div class="md-header-nav__scheme md-header-nav__button md-source__icon md-icon"> <a href=javascript:toggleScheme(); title="Light mode" class=light-mode> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg> </a> <a href=javascript:toggleScheme(); title="Dark mode" class=dark-mode> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg> </a> <a href=javascript:toggleScheme(); title="System preference" class=system-mode> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M7.5 2c-1.79 1.15-3 3.18-3 5.5s1.21 4.35 3.03 5.5C4.46 13 2 10.54 2 7.5A5.5 5.5 0 0 1 7.5 2m11.57 1.5 1.43 1.43L4.93 20.5 3.5 19.07 19.07 3.5m-6.18 2.43L11.41 5 9.97 6l.42-1.7L9 3.24l1.75-.12.58-1.65L12 3.1l1.73.03-1.35 1.13.51 1.67m-3.3 3.61-1.16-.73-1.12.78.34-1.32-1.09-.83 1.36-.09.45-1.29.51 1.27 1.36.03-1.05.87.4 1.31M19 13.5a5.5 5.5 0 0 1-5.5 5.5c-1.22 0-2.35-.4-3.26-1.07l7.69-7.69c.67.91 1.07 2.04 1.07 3.26m-4.4 6.58 2.77-1.15-.24 3.35-2.53-2.2m4.33-2.7 1.15-2.77 2.2 2.54-3.35.23m1.15-4.96-1.14-2.78 3.34.24-2.2 2.54M9.63 18.93l2.77 1.15-2.53 2.19-.24-3.34Z"/></svg> </a> <a href=javascript:toggleScheme(); title="Unknown scheme" class=unknown-mode> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m15.07 11.25-.9.92C13.45 12.89 13 13.5 13 15h-2v-.5c0-1.11.45-2.11 1.17-2.83l1.24-1.26c.37-.36.59-.86.59-1.41a2 2 0 0 0-2-2 2 2 0 0 0-2 2H8a4 4 0 0 1 4-4 4 4 0 0 1 4 4 3.2 3.2 0 0 1-.93 2.25M13 19h-2v-2h2M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10c0-5.53-4.5-10-10-10Z"/></svg> </a> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=.. class=md-tabs__link> TODO'S </a> </li> <li class=md-tabs__item> <a href=../10_organizer/ class=md-tabs__link> Organizer </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=./ class=md-tabs__link> Reuniões </a> </li> <li class=md-tabs__item> <a href=../3_ciafa/ class=md-tabs__link> CIAFA </a> </li> <li class=md-tabs__item> <a href=../4_datasets/ class=md-tabs__link> Datasets </a> </li> <li class=md-tabs__item> <a href=../5_links/ class=md-tabs__link> Links </a> </li> <li class=md-tabs__item> <a href=../7_workplan/ class=md-tabs__link> Workplan </a> </li> <li class=md-tabs__item> <a href=../8_yolov9/ class=md-tabs__link> YOLOv9 </a> </li> <li class=md-tabs__item> <a href=../9_pipeline/ class=md-tabs__link> Pipeline </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title="Tese Lima" class="md-nav__button md-logo" aria-label="Tese Lima" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m19 2-5 4.5v11l5-4.5V2M6.5 5C4.55 5 2.45 5.4 1 6.5v14.66c0 .25.25.5.5.5.1 0 .15-.07.25-.07 1.35-.65 3.3-1.09 4.75-1.09 1.95 0 4.05.4 5.5 1.5 1.35-.85 3.8-1.5 5.5-1.5 1.65 0 3.35.31 4.75 1.06.1.05.15.03.25.03.25 0 .5-.25.5-.5V6.5c-.6-.45-1.25-.75-2-1V19c-1.1-.35-2.3-.5-3.5-.5-1.7 0-4.15.65-5.5 1.5V6.5C10.55 5.4 8.45 5 6.5 5Z"/></svg> </a> Tese Lima </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> <span class=md-ellipsis> TODO'S </span> </a> </li> <li class=md-nav__item> <a href=../10_organizer/ class=md-nav__link> <span class=md-ellipsis> Organizer </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Reuniões </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Reuniões </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1-mar-24 class=md-nav__link> 1-Mar-24 </a> </li> <li class=md-nav__item> <a href=#8-mar-24 class=md-nav__link> 8-Mar-24 </a> </li> <li class=md-nav__item> <a href=#22-mar-24 class=md-nav__link> 22-Mar-24 </a> </li> <li class=md-nav__item> <a href=#26-apr-24 class=md-nav__link> 26-Apr-24 </a> </li> <li class=md-nav__item> <a href=#meeting-with-rashmi-27-apr-24 class=md-nav__link> Meeting with Rashmi 27-Apr-24 </a> </li> <li class=md-nav__item> <a href=#03-may-24-maj-cruz class=md-nav__link> 03-May-24 (Maj. Cruz) </a> </li> <li class=md-nav__item> <a href=#06-may-24-prof-bernardino class=md-nav__link> 06-May-24 (Prof. Bernardino) </a> <nav class=md-nav aria-label="06-May-24 (Prof. Bernardino)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#o-que-foi-feito-até-agora class=md-nav__link> O que foi feito até agora: </a> </li> <li class=md-nav__item> <a href=#problemas-a-discutir class=md-nav__link> Problemas a discutir: </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#09-may-24-maj-cruz class=md-nav__link> 09-May-24 (Maj. Cruz) </a> </li> <li class=md-nav__item> <a href=#13-may-24-trabalho class=md-nav__link> 13-May-24 (trabalho) </a> </li> <li class=md-nav__item> <a href=#23-may-24 class=md-nav__link> 23-May-24 </a> </li> <li class=md-nav__item> <a href=#06-jun-24-maj-cruz class=md-nav__link> 06-Jun-24 (Maj. Cruz) </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../3_ciafa/ class=md-nav__link> <span class=md-ellipsis> CIAFA </span> </a> </li> <li class=md-nav__item> <a href=../4_datasets/ class=md-nav__link> <span class=md-ellipsis> Datasets </span> </a> </li> <li class=md-nav__item> <a href=../5_links/ class=md-nav__link> <span class=md-ellipsis> Links </span> </a> </li> <li class=md-nav__item> <a href=../7_workplan/ class=md-nav__link> <span class=md-ellipsis> Workplan </span> </a> </li> <li class=md-nav__item> <a href=../8_yolov9/ class=md-nav__link> <span class=md-ellipsis> YOLOv9 </span> </a> </li> <li class=md-nav__item> <a href=../9_pipeline/ class=md-nav__link> <span class=md-ellipsis> Pipeline </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1-mar-24 class=md-nav__link> 1-Mar-24 </a> </li> <li class=md-nav__item> <a href=#8-mar-24 class=md-nav__link> 8-Mar-24 </a> </li> <li class=md-nav__item> <a href=#22-mar-24 class=md-nav__link> 22-Mar-24 </a> </li> <li class=md-nav__item> <a href=#26-apr-24 class=md-nav__link> 26-Apr-24 </a> </li> <li class=md-nav__item> <a href=#meeting-with-rashmi-27-apr-24 class=md-nav__link> Meeting with Rashmi 27-Apr-24 </a> </li> <li class=md-nav__item> <a href=#03-may-24-maj-cruz class=md-nav__link> 03-May-24 (Maj. Cruz) </a> </li> <li class=md-nav__item> <a href=#06-may-24-prof-bernardino class=md-nav__link> 06-May-24 (Prof. Bernardino) </a> <nav class=md-nav aria-label="06-May-24 (Prof. Bernardino)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#o-que-foi-feito-até-agora class=md-nav__link> O que foi feito até agora: </a> </li> <li class=md-nav__item> <a href=#problemas-a-discutir class=md-nav__link> Problemas a discutir: </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#09-may-24-maj-cruz class=md-nav__link> 09-May-24 (Maj. Cruz) </a> </li> <li class=md-nav__item> <a href=#13-may-24-trabalho class=md-nav__link> 13-May-24 (trabalho) </a> </li> <li class=md-nav__item> <a href=#23-may-24 class=md-nav__link> 23-May-24 </a> </li> <li class=md-nav__item> <a href=#06-jun-24-maj-cruz class=md-nav__link> 06-Jun-24 (Maj. Cruz) </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=reuniões>Reuniões<a class=headerlink href=#reuniões title="Permanent link"></a></h1> <h2 id=1-mar-24>1-Mar-24<a class=headerlink href=#1-mar-24 title="Permanent link"></a></h2> <p>YOLOv9 paper released on 21FEV24</p> <ul> <li>yolov9 + atrativo</li> <li>meter v9 a funcionar na maquina ciafa</li> <li>o q é q vamos ganhar em relação ao v8 ?</li> <li>perceber se v9 serve os nossos prpopositos ?</li> <li>exemplo minimo a correr no ciafa</li> <li>adotar escrito para tese</li> </ul> <h2 id=8-mar-24>8-Mar-24<a class=headerlink href=#8-mar-24 title="Permanent link"></a></h2> <ul> <li>03-Mar-24: eu percebi q estava a ser parvo quando disse "ah e tal o yolov9 n tem segmentação ainda, então talvez seja melhor ficar com o yolov8"</li> <li>para fazer o autolabelling nós apenas precisamos das capacidades do YOLO para fazer object detection, e por sua vez as "bounding boxes", portanto dá para usar YOLOv9</li> </ul> <h2 id=22-mar-24>22-Mar-24<a class=headerlink href=#22-mar-24 title="Permanent link"></a></h2> <ul> <li>equilibrio entre quantidade e qualidade de images</li> <li>selecionar datasets com o mais parecido com o que temos</li> <li>depois do YOLOv9 filtrar a qualidade do q vai para o SAM. basicamente ver se as bounding boxes estão boas para ir para o SAM e excluir o que tá errado</li> <li>ver ferramenta de anotação do roboflow para segmentação (Human annotator)</li> <li>unified dataset:</li> <li>prioridade UAV/aereo,</li> <li>identificar um test set de apenas imagens aereas anotadas e q não estejam no treino</li> <li>identificar outro test set com imagens genericas/ground anotadas sem estar no train set</li> <li>meter tb falso negativo</li> <li>treino pode ter coisas que não são aereo florestal</li> <li>~1000 test set (aéreas) (linha 7)</li> <li>~1000 test set + x (linha 8)</li> <li>datasets com frames fazer uma amostragem</li> <li>weights and biases</li> </ul> <h2 id=26-apr-24>26-Apr-24<a class=headerlink href=#26-apr-24 title="Permanent link"></a></h2> <ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> ver anchors do yolov9 (?)</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> ver configuaração que possa limitar tamanho de bounding box</li> <li>"As for bounding box size constraints, there isn't a direct built-in feature in YOLOv8 to limit detection by minimum or maximum bounding box size during the post-processing steps. However, it is a reasonable feature consideration for quality control in certain applications, and we may consider such additions for future versions." <a href=https://github.com/ultralytics/ultralytics/issues/3876#issuecomment-1811642710>source</a></li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> passar pipeline na dataset LK </li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> ver yolov8/v9 segmentacao</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> criar tabela com parametros do treino (tamanho de imagem, batch size, data augmentation, lr, epocas)</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> averiguar val a 0</li> <li></li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> guardar configuração do treino</li> </ul> <h2 id=meeting-with-rashmi-27-apr-24>Meeting with Rashmi 27-Apr-24<a class=headerlink href=#meeting-with-rashmi-27-apr-24 title="Permanent link"></a></h2> <details class=note> <summary>Meeting</summary> <div class="admonition question"> <p class=admonition-title>Do you upload your code to benchmarks?</p> <p>Yes, the code is uploaded to benchmarks, at least we do in MIT. We make our code open source to help other researches and to make our work reproducible.</p> </div> <div class="admonition info"> <p class=admonition-title>Info</p> <p>In <a href=https://paperswithcode.com/ >papers with code</a> there are multiple researches that upload their code and datasets. MIT and University of S. Francisco does this, but not every company or university does, such as CorsicanFire.</p> </div> <div class="admonition question"> <p class=admonition-title>How do you upload? For example, in a benchmark suite like CityScapes it is expected of the researcher to download a training set, develop a model, upload the model/code and then the model is evaluated by the benchmark suite on the backend. In my third challenge I want to give the option to researchers to distribute their code through different hardware, do you know any frameworks where this is possible?</p> <p>(maybe I didn't make myself very clear here because the answer was not very targeted to the hardware distribution) We usually use <a href=https://aws.amazon.com/sagemaker/ >Amazon SageMaker</a> <sup id=fnref:1><a class=footnote-ref href=#fn:1>1</a></sup> and <a href=https://aws.amazon.com/s3/ >Amazon S3</a> <sup id=fnref:2><a class=footnote-ref href=#fn:2>2</a></sup> bucket ecosystem. We upload our dataset do S3 bucket and our code to SageMaker. SageMaker is what runs the code files. This is <mark>not a free</mark> service </p> </div> <div class="admonition question"> <p class=admonition-title>Are you familiar with ONNX?</p> <p>Yes ONNX is heavily used. ONNX is also like a framework/platform is which you can have a checkpoint of a format of your model and then you can convert it to any other framework. For example, you can convert a PyTorch model to a TensorFlow model. ONNX is a very good platform for model conversion. This is called platform agnostic, so you can convert a model from one framework to another.</p> <p>For edge devices there is also TensorFlow Lite, which is a framework that is used to convert models to be used in edge devices. ONNX is more for cloud computing.</p> <ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> Look into ONNX</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> Look into TensorFlow Lite</li> </ul> </div> <div class="admonition question"> <p class=admonition-title>Can you elaborate on Amazon Sagemaker? How does it work?</p> <p>Basically Sagemaker is like when you use Google Colab GPU's. They offer very good GPU's and you can run your code on their GPU's. And S3 is like Google Drive, where you can store your data.</p> </div> <div class="admonition question"> <p class=admonition-title>You are explaining how your environment, as a developer/researcher, is right? Regarding to this, I was thinking to create a docker image that has pre-installed all the necessary libraries and frameworks to run the code and to make it compatible with the serverside hardware. Do you think this is a good idea?</p> <p>Yes docker is actually a good idea. Docker, Kubernetes and all these kind of platform in which you are adding whichever requirements are needed like the Python version, the libraries etc.</p> </div> <div class="admonition question"> <p class=admonition-title>Lets imagine that, you as a researcher, want to benchmark a model for fire and smoke segmentation and you come acros with my benchmark online. How would you like/be easier for you to interact with it?</p> <div class="admonition danger"> <p class=admonition-title><mark>Questão para os orientadores</mark> Is it even worth it to continue with the benchmark suite if there is already a popular and trusted platform named Kaggle?</p> </div> <p>You need the model from the researchers and you will give them a dataset. This is similar to Kaggle competitions. </p> <p>Unlike Kaggle, I don't know if it is possible to create a public repository where researchers can upload their models (upload the model on Amazon S3). </p> </div> <div class="admonition question"> <p class=admonition-title>Are there any free alternatives to Amazon S3 and SageMaker?</p> <p>Maybe Google Drive but that wouldnt be very efficient would it?</p> <p><mark>Me:</mark> I'm concerned about the size of the models if researchers are uploading them via Google Drive and also the privacy problems they might have on Google/Amazon services.</p> <p>Then, the best option would have to be a local server right? <mark>Does your university have a server that you can use?</mark></p> </div> <div class="admonition question"> <p class=admonition-title>To create the benchmark suite I was thinking to code a full stack application made with NuxtJS and Flask. Do you think this is a good idea?</p> <p>You would have problems with the storing of the models. You cant store them in, for example, a MySQL database because of the size. You would have to store them, either locally on the server, or in a cloud service like Amazon S3.</p> <p><mark>Me:</mark> Yes, I think we should store them locally. But we actually, as a benchmark, dont need to store the models and code for that long right? We only need the models to be stored for the time of the evaluation and then we could delete them, right?</p> <p><mark>Rashmi:</mark> Yes, you are right. You only need the models for the evaluation. I think your idea of using Docker images. I'm a bit skeptical for a single person to develop a full stack application like this because its very hard to do. You would normally need a team to do this. </p> </div> <div class="admonition question"> <p class=admonition-title>So the whole pipeline for this all it would be: I'm a researcher, I go to the benchmark suite website, I download the Docker image with the pre-defined libraries that need to run on the server-side I would develop my code. I would upload my code and my model and my results, after tested, would be displayed in a table</p> <p>Yes, it sounds simple but its very complex to implement. </p> </div> <div class="admonition question"> <p class=admonition-title>On the third challenge, I'm still not sure how could I make the splitting of the model/code throghout different hardware.</p> <p>What do you mean by different hardware? There would be hardware constraints? </p> <p><mark>Me:</mark> Yes, a challenge only on a NVIDIA 4090 and another challenge on a NVIDIA Jetson. Then a third challenge where a researcher can split the model across the two hardware.</p> <p><mark>Rashmi:</mark> Do you know the power of a NVIDIA Jetson? I'm not sure if it's capable of image processing. They can't process heavy images. What would be the size of the images? Are you aware about all these specs?</p> <p><mark>Me:</mark> I'm not sure about what hardware I will use. I know for a fact that I want to make a benchmark for normal GPU's, for edge devices and for a combination of both. What I was asked was to create a dataset for a benchmark suite, and I've been given a bunch of images, some labeled and some not. Then I should preform all the labelling on the images and create a unified dataset. The images I've been given are SUPER random, from frames of a video to aerial images to ground images, images from fireman, images from the web. The size of the images also differs a lot. </p> <p><mark>Rashmi:</mark> The image size for researchers is very important. There are images with 15MB and images with 2GB, and you can imagine that you cant process accurately a 2GB image in a model trained with 15MB images. </p> <p>It's crucial to consider the resources available to researchers. Sometimes, they might not have the means to train models or build them from scratch. From my perspective as an evaluator, I'll be assessing their models and benchmarking them. But it's important for researchers to also think about building their own models. (pelo contexto acho que ela estava a falar sobre a necessidade de nos preocuparmos com os recursos em termos de hardware dos investigadores, mas não percebi muito bem. Os pesos não iriam ser os mesmos se o código e dataset fosse o mesmo?)</p> <p>When <mark>discussing the dataset with your professor, consider what it will look like and what hardware resources</mark> you'll have access to. This information will be essential for building the front-end application. For instance, if you're planning to create a user interface with upload buttons, you need to know what kind of hardware you'll be working with.</p> <p>If you're dealing with multiple hardware options, things can get tricky. Not all hardware supports inference. For example, while we might use Amazon SageMaker for training models due to its capabilities, we often resort to EC2 instances for inference because they're more cost-effective.</p> <p>So, it's crucial to clarify the hardware resources available, the nature and size of the dataset, and then devise a plan for the full-stack architecture. Given the constraints and costs associated with various platforms like S3, SageMaker, Google Cloud, or Azure, it seems like building a custom full-stack application tailored to your specific needs is the best approach.</p> <p>That's just my perspective on you we should proceed.</p> </div> </details> <h2 id=03-may-24-maj-cruz>03-May-24 (Maj. Cruz)<a class=headerlink href=#03-may-24-maj-cruz title="Permanent link"></a></h2> <ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> establecer o tamanho de imagens da dataset</li> <li>o trabalho dos investigadores no nosso caso tb é tratar dos dados </li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> o que é que a NVIDIA tem para converter um modelo para um edge device</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> começar a escrever o que já foi feito no autolabelling</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> descrever o processo de autolabelling</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> explicar o YOLOv9 e SAM genericamente<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> explicar as configurações de cada e explicar o output e input de cada </li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> criar diagramas a explicar (talvez)</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> marcar reuniao c prof. Bernardino</li> </ul> <p>uma ideia por paragrafo </p> <p>Sobre as datasets: </p> <ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> fazer 2 grupos: imagens limpas e imagens contaminadas</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> organizar por tamanho de imagem</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> marcas de agua </li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> fazer estatisticas (tipo qual % de imagens tem marca de agua)</li> </ul> <h2 id=06-may-24-prof-bernardino>06-May-24 (Prof. Bernardino)<a class=headerlink href=#06-may-24-prof-bernardino title="Permanent link"></a></h2> <h3 id=o-que-foi-feito-até-agora>O que foi feito até agora:<a class=headerlink href=#o-que-foi-feito-até-agora title="Permanent link"></a></h3> <ul> <li>Autolabelling tool </li> </ul> <p><a class=glightbox href=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-32-42.png data-type=image data-width=auto data-height=auto data-title="picture 0" data-desc-position=bottom><img alt="picture 0" src=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-32-42.png></a> </p> <ul> <li> <p>Mostrar resultados da autolabelling tool</p> </li> <li> <p>Ferramenta para organizar as imagens, ver <a href=https://hslima00.github.io/Tese_md/10_organizer/ >aqui</a> e talvez mostrar. <a href=https://github.com/hslima00/Image_organizer>Github</a></p> </li> <li>Reunião com investigadora afiliada do MIT para perceber +- como é que eles fazem as coisas</li> </ul> <h3 id=problemas-a-discutir>Problemas a discutir:<a class=headerlink href=#problemas-a-discutir title="Permanent link"></a></h3> <ul> <li>Tamanho das imagens<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> não é problema</li> </ul> </li> <li>Qualidade das imagens, ou seja, se é para contar com imagens contaminadas (marca de agua, imagens com baixa qualidade, etc)<ul class=task-list> <li><mark>R</mark> não meter water marks no test set</li> <li>damos tudo aos investigadores, mas na test set queremos aproximar o máximo à realidade</li> <li>focar em deteção de fogo e fumo aérea</li> <li>não usar HH_Gestosa_Fire_Segmentation na training set, é uma boa dataset para test set </li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> fazer crop das imagens da HH_Gestosa_Fire_Segmentation par anão apanhar o aviao</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> ver as especificações da dataset, camara usada etc </li> </ul> </li> <li>Qual é o hardware que temos disponível<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> jetson edge </li> </ul> </li> <li>Como é que posso melhorar o YOLOv9 para o nosso caso de uso<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> experimentar modelo de segmentacao do YOLOv9 <mark>piores resultados</mark></li> </ul> </li> <li>IoU médio para a dataset GP_Fire_Segmentation_Webimages_v1 é de 0.52</li> <li>IoU médio para a dataset HH_Gestosa_Fire_Segmentation é de 0.00 -&gt; YOLO não detetou nada. </li> <li>Problemas do YOLOv9: falsos negativos, bounding boxes muito grandes<ul> <li>Existe maneira de diminuir as bounding boxes? (pelo que estive a ver nos issues do github do YOLOv8 não está implementada uma função do tipo)</li> </ul> </li> <li>Tenho duvidas sobre a segmentaçao de fumo por parte do SAM<ul> <li>fazer analise qualitativa, mostrar a varias pessoas a mesma segmentacao e perceber se elas concordam com a segmentação. Labels da Lisa têm dois graus, ambiguo e nao ambiguo</li> </ul> </li> <li>É possivel ter uma maquina remota do IST? Estou a usar uma da AFA e às vezes tenho problemas</li> </ul> <details class=note> <summary>Melhores 5 imagens da dataset GP_Fire_Segmentation_Webimages_v1</summary> <p><a class=glightbox href=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-03.png data-type=image data-width=auto data-height=auto data-title="picture 1" data-desc-position=bottom><img alt="picture 1" src=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-03.png></a><br> <a class=glightbox href=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-04.png data-type=image data-width=auto data-height=auto data-title="picture 2" data-desc-position=bottom><img alt="picture 2" src=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-04.png></a><br> <a class=glightbox href=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-05.png data-type=image data-width=auto data-height=auto data-title="picture 3" data-desc-position=bottom><img alt="picture 3" src=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-05.png></a><br> <a class=glightbox href=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-07.png data-type=image data-width=auto data-height=auto data-title="picture 4" data-desc-position=bottom><img alt="picture 4" src=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-07.png></a><br> <a class=glightbox href=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-08.png data-type=image data-width=auto data-height=auto data-title="picture 5" data-desc-position=bottom><img alt="picture 5" src=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-08.png></a></p> </details> <details class=note> <summary>Piores 5 imagens</summary> <p><a class=glightbox href=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-25.png data-type=image data-width=auto data-height=auto data-title="picture 6" data-desc-position=bottom><img alt="picture 6" src=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-25.png></a><br> <a class=glightbox href=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-26.png data-type=image data-width=auto data-height=auto data-title="picture 7" data-desc-position=bottom><img alt="picture 7" src=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-26.png></a><br> <a class=glightbox href=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-27.png data-type=image data-width=auto data-height=auto data-title="picture 8" data-desc-position=bottom><img alt="picture 8" src=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-27.png></a><br> <a class=glightbox href=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-28.png data-type=image data-width=auto data-height=auto data-title="picture 9" data-desc-position=bottom><img alt="picture 9" src=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-28.png></a><br> <a class=glightbox href=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-30.png data-type=image data-width=auto data-height=auto data-title="picture 10" data-desc-position=bottom><img alt="picture 10" src=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_06-05-2024_08-43-30.png></a> </p> </details> <h2 id=09-may-24-maj-cruz>09-May-24 (Maj. Cruz)<a class=headerlink href=#09-may-24-maj-cruz title="Permanent link"></a></h2> <p>Trabalho realizado esta semana: </p> <ul> <li>Avaliar se o YOLOv9-seg é melhor que a pipeline YOLOv9-det + SAM<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> treinado um modelo YOLOv9-seg com uma dataset com 201 imagens (foi o que encontrei preparado para YOLOv9&hellip;), o YOLOv9-det foi treinado com 8939&hellip;</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> retreinar com nova dataset com 6k images, para ser mais justo</li> </ul> </li> <li>Avaliado modelo treinado YOLOv9-det 8k images 100 epochs, para várias datasets, IoU subiu </li> <li>Detetado erro quando passagem da pipeline pela dataset HH_Gestosa, IoU está agora nos 0.4 relativamente à ground truth</li> <li>Passagem da pipeline pela LK_Fire com modelos diferentes, resultados parecem bons </li> </ul> <p>Duvidas: </p> <ul> <li>Ainda é possivel arranjar fotos da FAP? </li> </ul> <p>Trabalho Futuro:</p> <ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> Treinar YOLOv9-seg com 3k images</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> Ver se é melhor que a pipeline: <mark>não é melhor</mark></li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> Escrever sobre resultados: a escrever, ainda falta comparação entre pipeline e YOLOv9-seg </li> </ul> <p>Resultados:</p> <p><center></p> <table> <thead> <tr> <th>Dataset</th> <th>Model</th> <th>Method</th> <th>mean IoU</th> </tr> </thead> <tbody> <tr> <td>GP_Fire_Segmentation_Webimages_v1</td> <td>fire_best</td> <td>pipeline</td> <td>0.52</td> </tr> <tr> <td>GP_Fire_Segmentation_Webimages_v1</td> <td>5_out</td> <td>pipeline</td> <td>0.53</td> </tr> <tr> <td>LK_Fire+Smoke_V1</td> <td>fire_best</td> <td>pipeline</td> <td>no label</td> </tr> <tr> <td>LK_Fire+Smoke_V1</td> <td>5_out</td> <td>pipeline</td> <td>no label</td> </tr> <tr> <td>HH_Gestosa_Fire_Segmentation</td> <td>5_out</td> <td>pipeline</td> <td>0.4</td> </tr> <tr> <td>GP_Fire_Segmentation_Webimages_v1</td> <td>results_100</td> <td>yolov9-seg</td> <td>0.3</td> </tr> <tr> <td>GP_Fire_Segmentation_Webimages_v1</td> <td>results_200</td> <td>yolov9-seg</td> <td>0.33</td> </tr> </tbody> </table> <p></center></p> <h2 id=13-may-24-trabalho>13-May-24 (trabalho)<a class=headerlink href=#13-may-24-trabalho title="Permanent link"></a></h2> <ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> treinar YOLOv9-seg com 3k images 10 epochs</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> treinar YOLOv9-seg com 3k images 50 epochs</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> escrever sobre resultados</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> mandar mail ao Cmdt 991</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> pensar na forma de organizar as fotos </li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> conversão entre máscaras PB e label txt YOLOv9 <ul> <li>esta conversão só será útil para retreinar o yolov9 com as imagens que são anotadas pelo ser humano. para isto tenho de ponderar deixar a autolabeling OU arranjar uma forma de passar da máscara para uma label txt de bounding box (o que eu acho que é muito muito mais complexo)!</li> </ul> </li> </ul> <h2 id=23-may-24>23-May-24<a class=headerlink href=#23-may-24 title="Permanent link"></a></h2> <p><strong>Trabalho realizado:</strong></p> <ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> Pedido de fotos à Esq. 991 -&gt; perguntei ao Ten. Luís Santos informalmente se a esquadra tinha fotos, e ele disse que sim, porém que devia contactar o Cmdt. </li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> Pedido formal Sr. Gen. CEMFA</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> Treino YOLOv9-seg<ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> 10 epochs<ul> <li>3k images</li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> 50 epochs<ul> <li>3k images</li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> 100 epochs<ul> <li>200 images</li> <li>3k images</li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> 200 epochs<ul> <li>200 images</li> </ul> </li> </ul> </li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> Começar escrita</li> </ul> <p>Resultados YOLOv9-seg:</p> <p><center></p> <table> <thead> <tr> <th>Model</th> <th>IoU</th> </tr> </thead> <tbody> <tr> <td>10_epoch_model_3k</td> <td>0.4048</td> </tr> <tr> <td>50_epoch_model_3k</td> <td>0.429</td> </tr> <tr> <td>100_epoch_model_200</td> <td>0.307</td> </tr> <tr> <td>100_epoch_model_3k</td> <td>0.429</td> </tr> <tr> <td>200_epoch_model</td> <td>0.334</td> </tr> </tbody> </table> <p></center></p> <details class=note> <summary>Imagens</summary> <p><a class=glightbox href=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_23-05-2024_01-01-04.png data-type=image data-width=auto data-height=auto data-title="BBox e Segmentação" data-desc-position=bottom><img alt="BBox e Segmentação" src=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_23-05-2024_01-01-04.png></a> </p> <p><a class=glightbox href=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_23-05-2024_01-01-34.png data-type=image data-width=auto data-height=auto data-title="Mascara final" data-desc-position=bottom><img alt="Mascara final" src=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_23-05-2024_01-01-34.png></a> </p> <p><a class=glightbox href=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_23-05-2024_00-59-50.png data-type=image data-width=auto data-height=auto data-title="Diferença para ground truth" data-desc-position=bottom><img alt="Diferença para ground truth" src=https://cdn.statically.io/gh/hslima00/tese_md_images/main/2_reunioes_23-05-2024_00-59-50.png></a> </p> </details> <p><strong>Trabalho futuro:</strong></p> <ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> escrever sobre resultados do YOLOv9-seg</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> criar script para transformar mascaras PB em label txt YOLOv9, para retreinar </li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> treinar novo modelo com as imagens que já tenho label</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> começar a fazer a label das imagens</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> criar script para separar as imagens que são aceites e as que não são aceites pelo anotador (tipo, um script que mostre as imagens, as imagens com a deteção do YOLOv9 e as mascaras de segmentação. Depois o anotador só tem de premir uma tecla para decidir se passa ou não. Caso passe, a imagem e a sua respetiva label é automaticamente movida para a pasta final da unified dataset, caso não passe, a imagem é automaticamente movida para uma pasta de imagens não aceites. Depois desta passagem, retreinar o YOLOv9 novamente com as imagens que foram aceites. Repetir o processo até temros as imagens todas labeled)</li> </ul> <p><strong>Duvidas:</strong></p> <ul> <li>Imagens CA</li> <li>Pq é q não é boa ideia pedir imagens à 991? (o pedido já seguiu porque já tinha falado com o DC sobre isso) <ul> <li>ter imagens que não são acessiveis ao publico, ou seja, as que forem cedidas pela 991/CA são uma otima fonte para o test set que irá estar no server side</li> </ul> </li> <li>Perante os resultados, deixo o YOLOv9-seg e foco apenas na pipeline (YOLOv9-det+SAM)? Relembrar que o YOLOv9-det foi treinado com 6k imagens, e o YOLOv9-seg com 3k imagens. Também seria muito mais fácil (ainda não sei porque ainda não pensei sobre este assunto a fundo) passar as mascaras PB para o YOLOv9-seg visto que aquilo guarda a posição dos pixeis, facilmente retirada de uma imagem, enquanto que o YOLOv9-det são coordenadas normalizadas <code>xyhw</code>. </li> </ul> <h2 id=06-jun-24-maj-cruz>06-Jun-24 (Maj. Cruz)<a class=headerlink href=#06-jun-24-maj-cruz title="Permanent link"></a></h2> <p><strong>Trabalho realizado:</strong></p> <ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> Unified dataset (HL_firefront)</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> Hypertune de parametros </li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> Modelo escolhido: 20_ABR_conf_20 -&gt; aumentar a conf threshold melhorou alguns casos das bbox serem mt grandes. mIoU de 0.456. </li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled checked><span class=task-list-indicator></span></label> Comecei a ver a parte do backend e do frontend.</li> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> Fazer alterações ao LaTeX apontadas pelo Major </li> </ul> <p><strong>Trabalho futuro:</strong></p> <ul class=task-list> <li class=task-list-item><label class=task-list-control><input type=checkbox disabled><span class=task-list-indicator></span></label> Fazer programa que mostra as imagens originais e as labels feitas pela pipeline. Utilizador pode descartar a anotação ou pode aceitar. As que forem descartadas, quero integrar a lib do labelme no python para que se possa desenhar poligonos onde a anotação não for boa. Para isto tb tenho de arranjar maneira de passar de mascara B&amp;W para formato labelme, para aproveitar alguma coisa que esteja bem da pipeline e fazer alterações minimas. </li> </ul> <p><strong>Duvidas/Dificuldades:</strong></p> <ul> <li>Fiquei "vidrado" no treino de hypertunning de parametros perdendo algum tempo que podia ter canalizado para o avanço da pipeline e/ou da fullstack application.</li> <li>Ns se já mostrei o "Image Organizer", mas ainda n avancei muito com aquilo. Estão separadas apenas 2000/7500.</li> <li>Imagens 991/CA</li> <li></li> <li> <p>tentar guardar a penultima epoca</p> </li> <li>desdobrar o trabalho futuro em dois</li> <li>labelme é um nice to have </li> <li>pensar como é que os utilizadores vao submeter </li> <li>focar na app</li> <li>nas mascaras de segmentação ter duas cores para classes diferentes</li> <li>pensar em propsota de uniformização </li> <li>ver se mascaras tem 1 ou 3 canais (0,128,255)</li> </ul> <p>Latex: - explicar como é que o yolov9 se encaixa na pipeline - revisao literatura tenta se dizer que x metodologia usar y tecnica - falar das adaptacoes que tivemos que fazer para fazer a pipeline - </p> <div class=footnote> <hr> <ol> <li id=fn:1> <p>Build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows&#160;<a class=footnote-backref href=#fnref:1 title="Jump back to footnote 1 in the text">&#8617;</a></p> </li> <li id=fn:2> <p>Scalable, secure, and versatile object storage solution for diverse data needs.&#160;<a class=footnote-backref href=#fnref:2 title="Jump back to footnote 2 in the text">&#8617;</a></p> </li> </ol> </div> <hr> <div class=md-source-file> <small> Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">June 6, 2024</span> </small> </div> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=footer.title> <a href=../10_organizer/ class="md-footer__link md-footer__link--prev" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Previous </span> Organizer </div> </div> </a> <a href=../3_ciafa/ class="md-footer__link md-footer__link--next" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Next </span> CIAFA </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright &copy; 2022 Hugo Lima </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> with emoji by <a href=https://github.com/twitter/twemoji target=_blank rel=noopener> Twemoji </a> </div> <div class=md-social> <a href=https://github.com/hslima00 target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["navigation.tabs", "navigation.top", "navigation.instant", "content.tabs.link"], "search": "../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../assets/javascripts/bundle.81fa17fe.min.js></script> <script src=https://unpkg.com/mermaid@9.1.7/dist/mermaid.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../assets/pymdownx-extras/extra-loader-d39d49fc.js></script> <script src=../assets/pymdownx-extras/extra-loader-q9CWAeV9.js></script> <script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": true, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});})</script></body> </html>